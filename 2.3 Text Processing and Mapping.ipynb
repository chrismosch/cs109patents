{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.3 Text Processing and Industry Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Constantin Knoll, Christopher Mosch, Rohan Thavarajah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook predicts for each patent the industry to which the patent most likely belongs. This is done in three steps. First, we transform the raw patent text data into the form that is required to perform Latent Dirichlet Allocation (LDA). In particular, we obtain the abstract from each patent and the nouns contained in the former. We focus on the abstracts because they are the most expressive part of the patent and thus most relevant for our purpose. Secondly, we extract topics from the patent data using the LDA implementation in gensim. In the third step, we map the LDA topics to industries and thereby obtain an industry prediction for each patent. The first part of the mapping is to input the nouns of the industry definitions into LDA. Thereby, we use the resulting relation of industries to topics to create a sparse matrix of the industry-topic relation. For each patent, we then multiply the sparse vector representing the relation of the patent to all topics with the industry-topic matrix and thereby obtain for each industry the probability that a patent belongs to this industry. Finally, this data and additional data such as the inventor company is put into a data frame, which is then saved to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Image](Data\\Images\\Workflow_2.3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Change Log](#Change Log)\n",
    "* [Setup](#Setup)\n",
    "\n",
    "\n",
    "* [Reading in Data](#Reading)\n",
    "* [Preparing Data for LDA](#Preparing)\n",
    "* [Performing LDA](#Performing)\n",
    "* [Mapping Patents to Industries](#Mapping)\n",
    "* [Saving to S3](#Saving)\n",
    "\n",
    "\n",
    "* [Appendix](#Appendix)\n",
    "    * [Debugging Spark code](#A1)\n",
    "    * [Industry-Topic Intersection](#A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "### v.1\n",
    "- initial build\n",
    "\n",
    "### v.2 \n",
    "- added code for Spark\n",
    "\n",
    "### v.3 \n",
    "- added code to load data from web\n",
    "\n",
    "### v.4\n",
    "- switched from `ntlk` to `pattern` due to issues of `nltk` with Spark\n",
    "\n",
    "### v.5\n",
    "- switched from `BeautifulSoup` to `lxml` for performance\n",
    "- mapping from topics to industries\n",
    "- saves results to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Setup'></a>\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some parts do not require Spark, the latter is needed to run the full notebook\n",
    "on local machine (via vagrant) amound of data that can be used limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform analysis on full data, we used a single r3.8xlarge instance, and the code can also be used on a cluster. After setting up AWS CLI and an EC2 SSH key pair, we used the following specifications for the instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vagrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\"\n",
    "\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set('spark.executor.memory', '4g')\n",
    "    )\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading in Data\n",
    "import os, requests, zipfile, StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Preparing Data for LDA\n",
    "from lxml import etree \n",
    "import collections\n",
    "from pattern.en import tag\n",
    "#import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "# LDA\n",
    "import gensim\n",
    "\n",
    "# Mapping Patents to Industries\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not requried by default since we use pattern instead of nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('maxent_treebank_pos_tagger')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In notebook *2.1 Google Patent Data*, we created zip files for each week that contain the patents as xml files. From these xml files, we now create a dictionary `data` with the file names as keys and the xml strings as values. So the signature of `data` looks like\n",
    "\n",
    "`{'US06334220-20020101.XML': '<?xml version=\"1.0\"...', 'US06334221-20020101.XML': '<?xml version=\"1.0\"...', ...}`.\n",
    "\n",
    "This dictionary can be created in three different ways from the zip files. First, if the data is contained in several zip files, a txt file with links to the zip files is used. The txt file used below, https://s3.amazonaws.com/cs109project/2002-2004.txt, contains one link in each row, as illustrated below\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "https://s3-us-west-1.amazonaws.com/ckpatents/2002/20020101.zip  \n",
    "https://s3-us-west-1.amazonaws.com/ckpatents/2002/20020108.zip  \n",
    "https://s3-us-west-1.amazonaws.com/ckpatents/2002/20020115.zip  \n",
    "https://s3-us-west-1.amazonaws.com/ckpatents/2002/20020122.zip  \n",
    "https://s3-us-west-1.amazonaws.com/ckpatents/2002/20020129.zip  \n",
    "https://s3-us-west-1.amazonaws.com/ckpatents/2002/20020205.zip  \n",
    "...  \n",
    "</center>\n",
    "\n",
    "From each zip file, all xml files contained in it are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 19.7 s, total: 3min 21s\n",
      "Wall time: 9min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data from web (given url to file with urls of zips)\n",
    "# e.g.'https://s3.amazonaws.com/cs109project/2004.txt')\n",
    "# for each zip: loads all xmls files into dictionary with key=filename\n",
    "\n",
    "urls = 'https://s3.amazonaws.com/cs109project/2002-2004.txt'\n",
    "rs = requests.get(urls)\n",
    "data = {}\n",
    "\n",
    "# loop through urls\n",
    "for url in rs.content.split('\\r\\n'):\n",
    "    #print url\n",
    "    r = requests.get(url)\n",
    "    if zipfile.is_zipfile(StringIO.StringIO(r.content)):\n",
    "        z = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "        xmls = [member for member in z.namelist() if os.path.splitext(member.lower())[1]=='.xml']\n",
    "        newdata = {os.path.basename(xml): z.open(xml).read() for xml in xmls}\n",
    "        data.update(newdata)\n",
    "    else:\n",
    "        print url+' contains no zip file.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, if the data is contained in a single zip file, this can be read in directly. Again the code gets any xml file contained in the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# load data from web (given url to zip file)\n",
    "# e.g. e.g.'https://s3.amazonaws.com/cs109project/Unpacked+Data.zip'\n",
    "# loads all xmls files from given zip into dictionary with key=filename\n",
    "\n",
    "url = 'https://s3.amazonaws.com/cs109project/Unpacked+Data.zip'\n",
    "r = requests.get(url)\n",
    "if zipfile.is_zipfile(StringIO.StringIO(r.content)):\n",
    "    z = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "    xmls = [member for member in z.namelist() if os.path.splitext(member.lower())[1]=='.xml']\n",
    "    data = {os.path.basename(xml): z.open(xml).read() for xml in xmls}\n",
    "else:\n",
    "    print 'URL does not link to zip file. Please provide valid URL.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, data can be read in from disk where a nested structure with two levels is assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from disk\n",
    "# loads data with structure year\\weeks\\xmls into dictionary with key=filename\n",
    "source = os.getcwd()\n",
    "path = os.path.join(source,'2014')\n",
    "data = {}\n",
    "for week in os.listdir(path):\n",
    "    week_path = os.path.join(path, week)\n",
    "    for patent in os.listdir(week_path):\n",
    "        patent_path = os.path.join(week_path, patent)\n",
    "        if os.path.isfile(patent_path):\n",
    "            with open(patent_path, 'r') as my_file:\n",
    "                data[patent] = my_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preparing'></a>\n",
    "## Preparing Data for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract topics from patents, we need to obtain a list of nouns for each patent. We do this in two steps. First, we a get the abstract of each patent, which we because they are the most expressive part and thus most relevant for our analysis. Secondly, we get the nouns from each abstract and save them in the corpus form that `gensim`, the topic modelling library we use, requires.\n",
    "\n",
    "Moreover, in the first step we need to account for the fact that the structure of the xml files has changed at the end of 2004, as illustrated below. The image at the top shows the structure before the change while the image at the bottom displays the structure after the change.\n",
    "\n",
    "Before change (2004 and earlier)\n",
    "\n",
    "![Image](Data\\Images\\2.3 2004-.png?raw=true)\n",
    "\n",
    "After change (2005 and later)\n",
    "\n",
    "![Image](Data\\Images\\2.3 2005+.png?raw=true)\n",
    "\n",
    "To account for this change, the functions below take the parameter `xml_format`, where the value 'pre2005' indicates the first format. Since we will use additional information such as an inventor's geography and company for the analysis later, we also define a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_abstract_soup(xml, xml_format='pre2005'):\n",
    "    if xml_format=='pre2005':\n",
    "        # .strip() to get rid of the tags that are between the abstract and the entrance point ('sdoab') that is used\n",
    "        abstract = BeautifulSoup(xml, 'lxml').find('sdoab').get_text().strip()\n",
    "    else:\n",
    "        abstract = BeautifulSoup(xml, 'lxml').find('abstract').get_text()\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_abstract_lxml(xml, xml_format='pre2005'):\n",
    "    try:\n",
    "        if xml_format=='pre2005':\n",
    "            xml = xml.replace('&','')  #to avoid issues caused by the way special characters are saved 2004 and earlier\n",
    "            abstract = etree.XML(xml).xpath('//SDOAB//PDAT')[0].text\n",
    "        else:\n",
    "            abstract = etree.XML(xml).xpath('//abstract')[0][0].text\n",
    "\n",
    "        if not abstract:\n",
    "            #lxml less robust than soup (e.g. lxml returns None for US08623623-20140107.XML)\n",
    "            abstract = get_abstract_soup(xml, xml_format)\n",
    "    except:\n",
    "        abstract=''\n",
    "        \n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_supplement_lxml(xml, xml_format='pre2005'):\n",
    "    \n",
    "    if xml_format=='pre2005':\n",
    "        xml = xml.replace('&','')   #to avoid issues caused by the way special characters are saved 2004 and earlier\n",
    "        city_tag = '//CITY//PDAT'\n",
    "        state_tag = '//STATE//PDAT'\n",
    "        assignee_tag = '//ONM//PDAT'\n",
    "    else:\n",
    "        city_tag = '//city'\n",
    "        state_tag = '//state'\n",
    "        assignee_tag = '//assignee//orgname'        \n",
    "        \n",
    "    # inventor geography \n",
    "    try:\n",
    "        inv_city = etree.XML(xml).xpath(city_tag)[0].text\n",
    "    except:\n",
    "        inv_city = None\n",
    "\n",
    "    try:\n",
    "        inv_state = etree.XML(xml).xpath(state_tag)[0].text\n",
    "    except:\n",
    "        inv_state = None\n",
    "\n",
    "    if inv_state:\n",
    "        try:\n",
    "            if xml_format=='pre2005':\n",
    "                inv_ctry = etree.XML(xml).xpath(state_tag)[0].getnext().getchildren()[0].text\n",
    "            else:\n",
    "                inv_ctry = etree.XML(xml).xpath(state_tag)[0].getnext().text\n",
    "        except:\n",
    "            inv_ctry = None\n",
    "    else:\n",
    "        try:\n",
    "            if xml_format=='pre2005':\n",
    "                inv_ctry = etree.XML(xml).xpath('//CITY')[0].getnext().getchildren()[0].text\n",
    "            else:\n",
    "                inv_ctry = etree.XML(xml).xpath(city_tag)[0].getnext().text            \n",
    "        except:\n",
    "            inv_ctry = None\n",
    "\n",
    "    # inventor company        \n",
    "    try:\n",
    "        assignee = etree.XML(xml).xpath(assignee_tag)[0].text\n",
    "    except:\n",
    "        assignee = None\n",
    "    return [inv_city, inv_state, inv_ctry, assignee]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustrates the usage of these functions and their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rolling Hills', 'CA', None, 'Ledtronics, Inc.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_supplement_lxml(data[data.keys()[1]])#, xml_format='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A light source in the form of a light emitting diode (LED) cluster module suitable for use as an aircraft forward position light source. The light source comprises multiple LED components mounted on a base structure together with supporting electronic components to regulate the function of the LED components. The LED components are configured on the base structure in a manner so as to be capable of complying with the Federal Aviation Regulations minimum light intensities or candela requirements and color specifications while in a preferred implementation using a traditional aircraft 28-volt power supply. Furthermore, the preferred implementations are capable of meeting stringent dimensional design criteria and therefore are suitably adaptable as replacement light sources for existing aircraft forward position light housings.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abstract_lxml(data[data.keys()[1]])#, xml_format='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'A light source in the form of a light emitting diode (LED) cluster module suitable for use as an aircraft forward position light source. The light source comprises multiple LED components mounted on a base structure together with supporting electronic components to regulate the function of the LED components. The LED components are configured on the base structure in a manner so as to be capable of complying with the Federal Aviation Regulations minimum light intensities or candela requirements and color specifications while in a preferred implementation using a traditional aircraft 28-volt power supply. Furthermore, the preferred implementations are capable of meeting stringent dimensional design criteria and therefore are suitably adaptable as replacement light sources for existing aircraft forward position light housings.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abstract_soup(data[data.keys()[1]])#, xml_format='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, in the second step we extract all nouns from the abstracts and turn them into the form that is required for LDA. Due to issues of nltk with AWS, we decided to switch to the library `pattern`. The issues are described in more detail in the appendix, and although we figured out a workaround, we continued to use `pattern` as it is significantly faster than the workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nouns(text):  \n",
    "    # using pattern\n",
    "    tagged = tag(text.lower(), tokenize=True)\n",
    "    \n",
    "    # using nltk (if statements only necessary on AWS)\n",
    "    #if '/tmp' not in nltk.data.path:\n",
    "    #    nltk.data.path.append('/tmp')\n",
    "    #if 'tokenizers' not in os.listdir('/tmp'):\n",
    "    #    nltk.download('punkt', '/tmp')\n",
    "    #    nltk.download('maxent_treebank_pos_tagger', '/tmp')\n",
    "    #    nltk.download('averaged_perceptron_tagger', '/tmp')\n",
    "    #tokenized = word_tokenize(text.lower())\n",
    "    #tagged = nltk.pos_tag(tokenized)\n",
    "    \n",
    "    nouns = [a for (a, b) in tagged if b == 'NN']\n",
    "    return nouns\n",
    "\n",
    "# for each noun in list of nouns: get its number and number of occurences in list\n",
    "def tocorpus(nouns,vocabulary):\n",
    "    count = collections.defaultdict(int) # to count number of occurences of a noun\n",
    "    for noun in nouns:\n",
    "        count[vocabulary[noun]] +=1  # for new nouns: creates new key, sets value to 1. for existing keys: increases value by 1\n",
    "    return count.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the functions discussed above to the xml files in our dictionary `data`. Thereby, we obtain the dictionary `id2word` and the list `corpval` of tuples, which are passed to the LDA in the next step. There are both Spark and non-Spark versions of the code to do this. Due the amounts of data that are involved, however, it is highly recommended to use Spark. Nevertheless, the non-spark version that was used for some initial testing can be found further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of partitions for rdd throughout notebook\n",
    "part = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# code requires Spark\n",
    "\n",
    "data_rdd = sc.parallelize(data.iteritems(),part)\n",
    "print data_rdd.getNumPartitions()\n",
    "\n",
    "data_nouns = (data_rdd.mapValues(lambda v: get_abstract_lxml(v))\n",
    "               .mapValues(lambda v: get_nouns(v))\n",
    ").cache()\n",
    "\n",
    "# associates all distinct words with a number\n",
    "vocabtups = (data_nouns.flatMap(lambda (k,v): v)\n",
    "             .distinct()\n",
    "             .zipWithIndex()\n",
    ")\n",
    "\n",
    "vocab = vocabtups.collectAsMap()                            #word-to-number dict\n",
    "id2word = vocabtups.map(lambda (x,y): (y,x)).collectAsMap() #number-to-word dict\n",
    "\n",
    "corpus = data_nouns.mapValues(lambda v: tocorpus(v, vocab))\n",
    "corpval = corpus.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since above code might fail due to insufficient resources, it is often useful to examine disk and memory usage, which can be done with the code below. Among other things, this analysis was useful to identify issues with our Spark configurations. For example, it helped us to recognize the necessity of setting `SPARK_EXECUTOR_DIRS=\"/mnt/spark/\"` and `SPARK_WORKER_DIR=\"/mnt/spark/\"` respectively, as otherwise spark is saving any data into the root directory instead of `mnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem        Inodes  IUsed     IFree IUse% Mounted on\r\n",
      "/dev/xvda1        655360 178241    477119   28% /\r\n",
      "devtmpfs        31485576    669  31484907    1% /dev\r\n",
      "tmpfs           31487830      1  31487829    1% /dev/shm\r\n",
      "/dev/xvdb      314572800   9232 314563568    1% /mnt\r\n",
      "/dev/xvdc      314572800     81 314572719    1% /mnt1\r\n"
     ]
    }
   ],
   "source": [
    "# on AWS/vagrant: check inodes\n",
    "! df -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "/dev/xvda1      9.8G  8.1G  1.7G  84% /\r\n",
      "devtmpfs        121G   68K  121G   1% /dev\r\n",
      "tmpfs           121G     0  121G   0% /dev/shm\r\n",
      "/dev/xvdb       300G  231M  300G   1% /mnt\r\n",
      "/dev/xvdc       300G  341M  300G   1% /mnt1\r\n"
     ]
    }
   ],
   "source": [
    "# on AWS/vagrant: check disk space\n",
    "! df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             total       used       free     shared    buffers     cached\r\n",
      "Mem:        245998      61609     184389          0         87       3184\r\n",
      "-/+ buffers/cache:      58337     187661\r\n",
      "Swap:            0          0          0\r\n"
     ]
    }
   ],
   "source": [
    "# on AWS/vagrant: check memory usage\n",
    "! free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The non-Spark code is significantly slower but useful for some quick testing on few amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# code does not require Spark\n",
    "\n",
    "data_parsed = {k: get_abstract_lxml(v) for (k,v) in data.iteritems()}\n",
    "data_nouns = {k: get_nouns(v) for (k,v) in data_parsed.iteritems()}\n",
    "\n",
    "# loops through all list of nouns and gets all nouns and creates set of them (set ->each noun only once)\n",
    "flat = {item for sublist in data_nouns.values() for item in sublist}\n",
    "\n",
    "vocab = dict(zip(flat,range(len(flat))))    #word-to-number dict \n",
    "id2word = dict(zip(range(len(flat)),flat))  #number-to-word dict\n",
    "\n",
    "corpus = {k: tocorpus(v,vocab) for (k,v) in data_nouns.iteritems()}\n",
    "corpval = corpus.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` and `corpval` should have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336352 336352\n",
      "106625\n"
     ]
    }
   ],
   "source": [
    "print len(data),len(corpval)\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Performing'></a>\n",
    "## Perfoming LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this step was not the bottleneck with respect to computation time, we used the nondistributed version of ldamodel. Examples for the topics obtained from the LDA can be expanded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 54s, sys: 45 s, total: 9min 39s\n",
      "Wall time: 9min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_topics=40\n",
    "lda2 = gensim.models.ldamodel.LdaModel(corpval, id2word=id2word, num_topics=num_topics, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.119*vehicle + 0.047*table + 0.040*card + 0.035*system + 0.024*tray + 0.022*game + 0.022*feature + 0.021*camera + 0.016*page + 0.015*format',\n",
       " u'0.226*image + 0.054*color + 0.050*radiation + 0.049*apparatus + 0.030*article + 0.028*imaging + 0.025*detector + 0.023*rail + 0.018*vector + 0.018*needle',\n",
       " u'0.099*composition + 0.076*weight + 0.056*resin + 0.056*polymer + 0.040*b + 0.035*zone + 0.028*invention + 0.025*resistance + 0.023*fabric + 0.021*component',\n",
       " u'0.092*set + 0.088*value + 0.062*point + 0.044*sample + 0.038*pad + 0.036*video + 0.035*method + 0.029*pixel + 0.026*strip + 0.021*stream',\n",
       " u'0.314*device + 0.119*region + 0.095*area + 0.059*display + 0.053*type + 0.026*screen + 0.025*matrix + 0.014*bond + 0.014*conductivity + 0.010*n',\n",
       " u'0.133*heat + 0.105*container + 0.062*transistor + 0.045*injection + 0.039*heating + 0.034*food + 0.027*compartment + 0.022*lid + 0.021*dispenser + 0.019*exchange',\n",
       " u'0.130*water + 0.072*coating + 0.057*solution + 0.033*content + 0.027*plasma + 0.025*concentration + 0.024*method + 0.022*oxygen + 0.022*mass + 0.017*treatment',\n",
       " u'0.164*invention + 0.038*method + 0.035*formula + 0.032*treatment + 0.027*use + 0.027*vessel + 0.027*i + 0.024*production + 0.023*patient + 0.020*blood',\n",
       " u'0.071*network + 0.057*component + 0.056*communication + 0.045*module + 0.038*system + 0.034*station + 0.029*service + 0.028*message + 0.027*transmission + 0.023*interface',\n",
       " u'0.085*process + 0.054*group + 0.050*phase + 0.044*acid + 0.039*product + 0.035*compound + 0.034*agent + 0.028*reaction + 0.027*mixture + 0.026*catalyst',\n",
       " u'0.088*air + 0.076*section + 0.074*gas + 0.055*engine + 0.043*flow + 0.034*system + 0.031*fuel + 0.029*pressure + 0.024*combustion + 0.023*nozzle',\n",
       " u'0.239*material + 0.075*sheet + 0.049*core + 0.038*printing + 0.031*belt + 0.027*print + 0.025*paper + 0.024*box + 0.023*powder + 0.023*strength',\n",
       " u'0.086*object + 0.083*controller + 0.073*test + 0.057*disk + 0.035*probe + 0.033*measurement + 0.029*method + 0.028*detection + 0.027*apparatus + 0.027*shift',\n",
       " u'0.173*memory + 0.090*cell + 0.059*array + 0.040*bit + 0.039*bus + 0.035*plurality + 0.026*address + 0.025*column + 0.021*data + 0.017*access',\n",
       " u'0.148*temperature + 0.134*sensor + 0.101*pressure + 0.035*volume + 0.029*stage + 0.023*gain + 0.022*loop + 0.019*grid + 0.019*slide + 0.017*implant',\n",
       " u'0.132*end + 0.068*housing + 0.028*connector + 0.027*arm + 0.026*hole + 0.021*assembly + 0.019*spring + 0.019*rod + 0.019*wheel + 0.019*portion',\n",
       " u'0.073*block + 0.072*pattern + 0.055*liquid + 0.046*plane + 0.042*antenna + 0.041*size + 0.035*distribution + 0.034*angle + 0.024*enclosure + 0.023*projection',\n",
       " u'0.130*circuit + 0.107*part + 0.054*chip + 0.051*board + 0.040*connection + 0.033*semiconductor + 0.033*contact + 0.031*conductor + 0.031*package + 0.026*plurality',\n",
       " u'0.080*cable + 0.073*application + 0.050*tissue + 0.037*driver + 0.031*site + 0.030*% + 0.023*anode + 0.018*resource + 0.016*property + 0.016*installation',\n",
       " u'0.335*portion + 0.056*target + 0.042*transfer + 0.026*bag + 0.026*scanning + 0.024*tip + 0.022*piece + 0.021*model + 0.018*diameter + 0.015*toner',\n",
       " u'0.103*chamber + 0.082*valve + 0.078*tube + 0.061*filter + 0.051*fluid + 0.027*window + 0.027*segment + 0.025*vacuum + 0.023*delivery + 0.018*catheter',\n",
       " u'0.157*electrode + 0.109*panel + 0.057*crystal + 0.050*bar + 0.046*glass + 0.033*code + 0.031*electron + 0.027*alignment + 0.025*capacitor + 0.023*cathode',\n",
       " u'0.152*member + 0.085*surface + 0.066*support + 0.049*frame + 0.043*base + 0.026*seat + 0.026*pair + 0.025*assembly + 0.025*cover + 0.021*face',\n",
       " u'0.121*system + 0.088*information + 0.049*data + 0.045*user + 0.041*computer + 0.040*storage + 0.029*method + 0.019*server + 0.018*program + 0.018*web',\n",
       " u'0.106*time + 0.062*channel + 0.042*energy + 0.041*rate + 0.032*method + 0.028*period + 0.023*sequence + 0.020*timing + 0.020*number + 0.017*pulse',\n",
       " u'0.306*body + 0.104*head + 0.044*door + 0.025*shell + 0.024*document + 0.020*hook + 0.019*master + 0.019*neck + 0.018*waste + 0.017*resistor',\n",
       " u'0.082*step + 0.036*activity + 0.036*method + 0.035*motion + 0.034*roll + 0.031*index + 0.027*rubber + 0.025*reduction + 0.022*invention + 0.021*factor',\n",
       " u'0.117*light + 0.076*beam + 0.074*source + 0.061*medium + 0.055*laser + 0.053*state + 0.033*mode + 0.028*speed + 0.025*wavelength + 0.024*system',\n",
       " u'0.193*side + 0.109*wall + 0.046*case + 0.043*pipe + 0.032*band + 0.030*tape + 0.029*track + 0.029*cartridge + 0.024*membrane + 0.019*floor',\n",
       " u'0.200*plate + 0.073*carrier + 0.070*wafer + 0.053*cavity + 0.050*surface + 0.046*mold + 0.023*molding + 0.019*coefficient + 0.019*friction + 0.017*exposure',\n",
       " u'0.179*line + 0.079*path + 0.076*ink + 0.050*width + 0.042*length + 0.039*edge + 0.032*profile + 0.029*compression + 0.026*transport + 0.025*jet',\n",
       " u'0.141*power + 0.118*voltage + 0.078*circuit + 0.037*supply + 0.036*switch + 0.033*level + 0.031*terminal + 0.028*source + 0.026*reference + 0.021*battery',\n",
       " u'0.166*layer + 0.105*substrate + 0.054*film + 0.043*surface + 0.043*semiconductor + 0.038*metal + 0.033*gate + 0.032*structure + 0.028*method + 0.022*silicon',\n",
       " u'0.102*position + 0.058*mechanism + 0.055*assembly + 0.048*drive + 0.045*motor + 0.045*shaft + 0.034*tool + 0.028*cylinder + 0.027*machine + 0.025*rotation',\n",
       " u'0.288*unit + 0.189*control + 0.055*space + 0.030*lamp + 0.022*discharge + 0.020*operation + 0.017*foot + 0.016*operator + 0.016*x-ray + 0.013*system',\n",
       " u'0.052*plastic + 0.038*hand + 0.036*c. + 0.032*instruction + 0.031*percent + 0.026*vibration + 0.026*execution + 0.021*illumination + 0.020*pocket + 0.019*manufacture',\n",
       " u'0.239*element + 0.093*lens + 0.078*fiber + 0.044*rotor + 0.043*roller + 0.028*axis + 0.026*conveyor + 0.023*bone + 0.020*surface + 0.019*diode',\n",
       " u'0.079*load + 0.065*oil + 0.049*latch + 0.041*radio + 0.031*capacity + 0.030*clamp + 0.029*plant + 0.029*sense + 0.029*string + 0.028*impedance',\n",
       " u'0.097*field + 0.081*ring + 0.067*sleeve + 0.066*coil + 0.055*holder + 0.037*ground + 0.037*gap + 0.027*pole + 0.025*fan + 0.023*key',\n",
       " u'0.207*signal + 0.090*output + 0.079*input + 0.048*circuit + 0.042*frequency + 0.031*clock + 0.019*logic + 0.018*receiver + 0.017*error + 0.015*converter']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Mapping'></a>\n",
    "## Mapping Patents to Industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Industry Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch naics nouns\n",
    "naics_url = 'https://s3.amazonaws.com/cs109projectr/naics_nouns.json'\n",
    "naics_r = requests.get(naics_url)\n",
    "naics_nouns = json.loads(naics_r.content)\n",
    "\n",
    "# take only manufacturing\n",
    "industries =  [u'311',u'312',u'313',u'314',u'315',u'316',u'321',u'322',u'323',u'324',u'325',\n",
    "               u'326',u'327',u'331',u'332',u'333',u'334',u'335',u'336',u'337',u'339'\n",
    "              ]\n",
    "naics_subset = []\n",
    "for i in naics_nouns:\n",
    "    if i['naics_code'] in industries:\n",
    "        naics_subset.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    }
   ],
   "source": [
    "# check that no invalid naics code entered\n",
    "print len(industries), len(naics_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format naics data\n",
    "'''\n",
    "format_naics\n",
    "input = list of dictionaries with naics definitions (json output of ind definitions ipython notebook)\n",
    "output = naics_tuples, naics_codes\n",
    "naics_tuples = list of tuples, tuple1 = (noun1, count1), tuple2 = ()...\n",
    "naics_codes = list of naics codes\n",
    "'''\n",
    "def format_naics(input):\n",
    "    naics_tuples = [i['noun_dict'].items() for i in input]\n",
    "    naics_codes = [i['naics_code'] for i in input]\n",
    "    return naics_tuples, naics_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    }
   ],
   "source": [
    "naics_tuples, naics_codes = format_naics(naics_subset)\n",
    "print len(naics_tuples), len(naics_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace naics nouns with vocab id\n",
    "# i.e. preps naics definitions with the same signature as corpval\n",
    "naics_output = []\n",
    "for i in naics_tuples:\n",
    "    new_noun_list = []\n",
    "    for j in i:\n",
    "        try:\n",
    "            new_noun_list.append((vocab[j[0]], j[1]))\n",
    "        except:\n",
    "            pass\n",
    "            #print j[0]\n",
    "    naics_output.append(new_noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sparse matrix of industry-topic relation\n",
    "val = []\n",
    "col = []\n",
    "row = []\n",
    "for i,industry in enumerate(naics_output):\n",
    "    for topic in lda2.get_document_topics(industry):\n",
    "        row.append(i) # only changes after len(industry) numbers of loops\n",
    "        col.append(topic[0]) # index of topic\n",
    "        val.append(topic[1]) # probability that industry belong to this topic\n",
    "# spark doc recommends np.array over list for efficiency (http://spark.apache.org/docs/latest/mllib-data-types.html)\n",
    "indstr_tpc = sps.csc_matrix((np.array(val), (np.array(row), np.array(col))), shape=(len(industries), num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03742877,  0.01135007,  0.        ,  0.02473819,  0.02121565,\n",
       "         0.01255455,  0.02051569,  0.        ,  0.04160644,  0.01745859,\n",
       "         0.02478754,  0.01134821,  0.12848977,  0.02547809,  0.02672588,\n",
       "         0.        ,  0.01314119,  0.02333849,  0.01169152,  0.        ,\n",
       "         0.015762  ,  0.02421776,  0.        ,  0.06768398,  0.04058414,\n",
       "         0.        ,  0.        ,  0.01483675,  0.02052445,  0.        ,\n",
       "         0.        ,  0.0275468 ,  0.0196355 ,  0.01645676,  0.02288635,\n",
       "         0.18732518,  0.        ,  0.01367302,  0.        ,  0.01077511],\n",
       "       [ 0.10917954,  0.        ,  0.        ,  0.01375668,  0.        ,\n",
       "         0.        ,  0.        ,  0.01833028,  0.        ,  0.        ,\n",
       "         0.06726431,  0.        ,  0.03188074,  0.06607134,  0.        ,\n",
       "         0.03609818,  0.0678537 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.01585597,  0.0197514 ,  0.01150197,  0.        ,  0.01042443,\n",
       "         0.01588586,  0.        ,  0.        ,  0.05163305,  0.        ,\n",
       "         0.02117071,  0.01336866,  0.01960408,  0.13858922,  0.03887245,\n",
       "         0.12350424,  0.        ,  0.        ,  0.01388552,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.01423619,  0.        ,\n",
       "         0.        ,  0.13560237,  0.11653734,  0.        ,  0.03116762,\n",
       "         0.        ,  0.0312902 ,  0.        ,  0.03667522,  0.        ,\n",
       "         0.04524389,  0.        ,  0.        ,  0.01261861,  0.        ,\n",
       "         0.02280738,  0.0375533 ,  0.07408022,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.05765264,  0.02060427,\n",
       "         0.        ,  0.        ,  0.14821306,  0.        ,  0.        ,\n",
       "         0.13090161,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.02523828,  0.        ,  0.1657968 ,  0.        ,  0.        ,\n",
       "         0.01083359,  0.03500885,  0.04566287,  0.10759023,  0.        ,\n",
       "         0.01059577,  0.05301221,  0.01104701,  0.04035261,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.04995365,  0.        ,  0.        ,\n",
       "         0.09702099,  0.0106044 ,  0.        ,  0.01623191,  0.        ,\n",
       "         0.01233454,  0.        ,  0.        ,  0.01231095,  0.02295892,\n",
       "         0.08816277,  0.11068941,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.03149669,  0.        ,  0.10858195,\n",
       "         0.12425855,  0.09883784,  0.02104866,  0.0103342 ,  0.04302886,\n",
       "         0.        ,  0.01053759,  0.02279124,  0.06740111,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.03836314,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.01605179,  0.01092372,  0.        ,  0.01576875,  0.        ,\n",
       "         0.02361778,  0.        ,  0.06854406,  0.        ,  0.        ,\n",
       "         0.25973697,  0.        ,  0.01095202,  0.        ,  0.        ],\n",
       "       [ 0.01037474,  0.0225788 ,  0.02079154,  0.        ,  0.        ,\n",
       "         0.10777039,  0.04926379,  0.02451681,  0.02778922,  0.02941929,\n",
       "         0.        ,  0.01914092,  0.03017178,  0.03860124,  0.04450927,\n",
       "         0.        ,  0.01409826,  0.        ,  0.01466973,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.07828595,  0.0251288 ,  0.        ,  0.013338  ,  0.01369006,\n",
       "         0.06932861,  0.01750647,  0.01734534,  0.01024481,  0.        ,\n",
       "         0.16704912,  0.        ,  0.02523966,  0.03505971,  0.02982251],\n",
       "       [ 0.01045438,  0.        ,  0.06332626,  0.        ,  0.        ,\n",
       "         0.01297434,  0.        ,  0.24048051,  0.        ,  0.01212325,\n",
       "         0.        ,  0.        ,  0.        ,  0.02568109,  0.        ,\n",
       "         0.0157773 ,  0.        ,  0.        ,  0.0419386 ,  0.01632811,\n",
       "         0.        ,  0.        ,  0.0107279 ,  0.        ,  0.        ,\n",
       "         0.01760627,  0.07992777,  0.        ,  0.03739158,  0.        ,\n",
       "         0.        ,  0.        ,  0.05756665,  0.        ,  0.        ,\n",
       "         0.26371129,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.15692945,  0.01317979,  0.        ,\n",
       "         0.        ,  0.        ,  0.04697518,  0.01349096,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.02742126,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.02417904,  0.01642326,\n",
       "         0.        ,  0.        ,  0.        ,  0.01543024,  0.        ,\n",
       "         0.32541384,  0.0317248 ,  0.        ,  0.        ,  0.01383664,\n",
       "         0.        ,  0.        ,  0.0293705 ,  0.        ,  0.01824003,\n",
       "         0.18724627,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.04239549,  0.        ,  0.07079175,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.03115193,  0.01751078,  0.03443017,\n",
       "         0.02472661,  0.0315346 ,  0.01729658,  0.05968643,  0.01726319,\n",
       "         0.        ,  0.01390826,  0.        ,  0.03812045,  0.01037627,\n",
       "         0.01715757,  0.        ,  0.02382952,  0.        ,  0.        ,\n",
       "         0.22463159,  0.04140635,  0.        ,  0.02759648,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.17918162,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.03284403,  0.01125565,  0.01819354,  0.01139122,  0.03111108,\n",
       "         0.01876518,  0.0162812 ,  0.04218188,  0.01675746,  0.        ,\n",
       "         0.        ,  0.03943032,  0.0515633 ,  0.03143662,  0.        ,\n",
       "         0.01282719,  0.01017654,  0.        ,  0.02694757,  0.01675991,\n",
       "         0.01399907,  0.        ,  0.02855389,  0.        ,  0.        ,\n",
       "         0.05074047,  0.        ,  0.        ,  0.02049822,  0.01225449,\n",
       "         0.        ,  0.        ,  0.08035715,  0.        ,  0.        ,\n",
       "         0.27352785,  0.01616369,  0.        ,  0.02094502,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01321589,  0.        ,  0.        ,\n",
       "         0.02050182,  0.        ,  0.01645311,  0.02930805,  0.0273439 ,\n",
       "         0.04145896,  0.        ,  0.05532073,  0.06382891,  0.01795584,\n",
       "         0.01052978,  0.01889709,  0.04342752,  0.01705595,  0.        ,\n",
       "         0.01909563,  0.02945979,  0.02593384,  0.        ,  0.01462368,\n",
       "         0.        ,  0.        ,  0.01307473,  0.02144918,  0.        ,\n",
       "         0.        ,  0.1168311 ,  0.01727379,  0.01317588,  0.04510444,\n",
       "         0.19740562,  0.        ,  0.        ,  0.02856551,  0.        ],\n",
       "       [ 0.0299511 ,  0.        ,  0.02669418,  0.03013385,  0.15183533,\n",
       "         0.        ,  0.        ,  0.01512588,  0.16523017,  0.01232122,\n",
       "         0.        ,  0.01414793,  0.        ,  0.02307872,  0.        ,\n",
       "         0.        ,  0.01488581,  0.        ,  0.        ,  0.        ,\n",
       "         0.01322695,  0.        ,  0.03241522,  0.01002186,  0.        ,\n",
       "         0.04041645,  0.        ,  0.        ,  0.04167946,  0.        ,\n",
       "         0.        ,  0.        ,  0.09052383,  0.02233993,  0.        ,\n",
       "         0.13615664,  0.        ,  0.0119327 ,  0.03590056,  0.        ],\n",
       "       [ 0.01929582,  0.        ,  0.        ,  0.01029166,  0.        ,\n",
       "         0.02542543,  0.02166092,  0.        ,  0.01719983,  0.04393686,\n",
       "         0.04144733,  0.03124908,  0.06468722,  0.03515144,  0.        ,\n",
       "         0.02155841,  0.01158375,  0.        ,  0.        ,  0.11628653,\n",
       "         0.        ,  0.01111719,  0.01538985,  0.01548238,  0.        ,\n",
       "         0.01401514,  0.        ,  0.        ,  0.02341441,  0.        ,\n",
       "         0.        ,  0.02136653,  0.03042822,  0.05272714,  0.        ,\n",
       "         0.21715583,  0.        ,  0.        ,  0.019454  ,  0.        ],\n",
       "       [ 0.01292086,  0.        ,  0.0132427 ,  0.02912211,  0.        ,\n",
       "         0.01646135,  0.02874543,  0.01148253,  0.        ,  0.01802613,\n",
       "         0.01615102,  0.0353834 ,  0.        ,  0.03919811,  0.        ,\n",
       "         0.01740014,  0.        ,  0.        ,  0.02378212,  0.        ,\n",
       "         0.02478527,  0.        ,  0.05370668,  0.01246797,  0.        ,\n",
       "         0.02240029,  0.0483371 ,  0.        ,  0.03243365,  0.02222491,\n",
       "         0.01555682,  0.01682183,  0.25386222,  0.02333251,  0.01349321,\n",
       "         0.12097075,  0.01175743,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.02690571,  0.03960175,  0.        ,  0.01386587,  0.01134295,\n",
       "         0.        ,  0.36560467,  0.        ,  0.03962565,  0.        ,\n",
       "         0.        ,  0.        ,  0.02567197,  0.        ,  0.01285588,\n",
       "         0.02582854,  0.        ,  0.0138226 ,  0.13727827,  0.        ,\n",
       "         0.02363114,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.0235317 ,  0.01088577,  0.04740111,  0.        ,  0.        ,\n",
       "         0.08033826,  0.02490721,  0.        ,  0.0171659 ,  0.        ],\n",
       "       [ 0.0347616 ,  0.01125233,  0.        ,  0.0322185 ,  0.1341404 ,\n",
       "         0.        ,  0.        ,  0.04592116,  0.01013114,  0.        ,\n",
       "         0.0401993 ,  0.19427621,  0.        ,  0.06109528,  0.01671632,\n",
       "         0.        ,  0.01202008,  0.        ,  0.02434235,  0.06388904,\n",
       "         0.        ,  0.        ,  0.0385386 ,  0.09557892,  0.        ,\n",
       "         0.02224265,  0.        ,  0.        ,  0.        ,  0.02708712,\n",
       "         0.        ,  0.        ,  0.02130517,  0.01039747,  0.        ,\n",
       "         0.04598212,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.0118905 ,  0.02974385,  0.29954698,\n",
       "         0.        ,  0.        ,  0.        ,  0.01625915,  0.01834639,\n",
       "         0.        ,  0.        ,  0.01756253,  0.02446525,  0.        ,\n",
       "         0.01870022,  0.        ,  0.        ,  0.01265685,  0.01782607,\n",
       "         0.02173232,  0.        ,  0.0143305 ,  0.        ,  0.        ,\n",
       "         0.03273457,  0.        ,  0.        ,  0.06331839,  0.03986802,\n",
       "         0.0245125 ,  0.        ,  0.0395264 ,  0.        ,  0.02963702,\n",
       "         0.15937241,  0.        ,  0.        ,  0.01295937,  0.        ],\n",
       "       [ 0.02126563,  0.        ,  0.03069121,  0.        ,  0.        ,\n",
       "         0.01259595,  0.0196462 ,  0.01317943,  0.        ,  0.06110446,\n",
       "         0.        ,  0.04733412,  0.        ,  0.04246428,  0.        ,\n",
       "         0.01443209,  0.        ,  0.04374636,  0.        ,  0.        ,\n",
       "         0.02991461,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.0146772 ,  0.13831606,  0.        ,  0.05530975,  0.02431048,\n",
       "         0.        ,  0.        ,  0.06850115,  0.014065  ,  0.        ,\n",
       "         0.2008494 ,  0.06200182,  0.        ,  0.013183  ,  0.        ],\n",
       "       [ 0.02967691,  0.        ,  0.05523428,  0.        ,  0.        ,\n",
       "         0.        ,  0.13189338,  0.        ,  0.        ,  0.03825898,\n",
       "         0.        ,  0.01086294,  0.        ,  0.03528255,  0.        ,\n",
       "         0.        ,  0.03956661,  0.01458758,  0.        ,  0.        ,\n",
       "         0.        ,  0.17028679,  0.01239436,  0.        ,  0.        ,\n",
       "         0.02297242,  0.        ,  0.        ,  0.04186147,  0.01559787,\n",
       "         0.        ,  0.        ,  0.01905269,  0.02745959,  0.01935878,\n",
       "         0.17905579,  0.        ,  0.        ,  0.02544696,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01777764,  0.        ,  0.        ,\n",
       "         0.        ,  0.41423956,  0.        ,  0.        ,  0.07221346,\n",
       "         0.01780507,  0.02583826,  0.        ,  0.08097671,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.1581802 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.06962131,  0.01112315,  0.06626851,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.04780505,  0.        ,  0.01557927,\n",
       "         0.01377039,  0.06160997,  0.04917161,  0.01253073,  0.14525001,\n",
       "         0.01018014,  0.02079893,  0.        ,  0.04193926,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01374932,\n",
       "         0.        ,  0.        ,  0.01130761,  0.        ,  0.        ,\n",
       "         0.01872997,  0.05720545,  0.        ,  0.01177617,  0.        ,\n",
       "         0.01185339,  0.        ,  0.01694252,  0.        ,  0.02588842,\n",
       "         0.2737223 ,  0.01594374,  0.0304138 ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indstr_tpc.toarray() # sanity check: compare this to [lda2.get_document_topics(i) for i in naics_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to the approach described above, one could take the intersection between the nouns associated with each LDA topic and the nouns of the industry's definitions. For a particular topic, the industry whose definition has the largest intersection with this topic would then be assumed to be the industry that the topic represents. This process is illustrated in the Appendix of this notebook (note that SIC instead of NAICS are used as industry definitions). Another version of the intersection approach would be to directly take the intersection of the nouns of industry defitions and the nouns of patent abstract. However, because a single patent is very spezialized compared to the scope of a whole industry, we prefered the  approach used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent-Industry Relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for Single Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input\n",
    "nouns: list of nouns with signature as corpval\n",
    "lda: trained lda classifier\n",
    "indstr_tpc_matrix: matrix of relation between industries and topics (dimesions: #indutries x # topics)\n",
    "\n",
    "output\n",
    "list of probabilities for each topic\n",
    "'''\n",
    "def tpc2indstr(nouns, lda, indstr_tpc_matrix):\n",
    "    topics = lda.get_document_topics(nouns)\n",
    "    \n",
    "    # turn topic probabilities into sparse row vector\n",
    "    val = []\n",
    "    col = []\n",
    "    for topic in topics:\n",
    "        col.append(topic[0]) # index of topic\n",
    "        val.append(topic[1]) # probability that industry belong to this topic\n",
    "    tpc_vector = sps.csc_matrix((np.array(val), (np.zeros(len(val)), np.array(col))), shape=(1, num_topics))\n",
    "    \n",
    "    # combine probabilities with relation between topics and industries\n",
    "    indstr = indstr_tpc_matrix * tpc_vector.transpose()\n",
    "    \n",
    "    # normalize such that sum = 1 -> interpretable as probability\n",
    "    indstr_norm = indstr / indstr.sum()\n",
    "    \n",
    "    # convert sparse matrix into array into flattened list\n",
    "    return indstr_norm.transpose().toarray().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on single instance: grant permission\n",
    "! sudo chmod -R ugo+rw /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recall: corpval = corpus.values().collect()\n",
    "instrs = corpus.mapValues(lambda v: tpc2indstr(v, lda2, indstr_tpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input\n",
    "nouns: list of nouns with signature as corpval\n",
    "lda: trained lda classifier\n",
    "indstr_tpc_matrix: matrix of relation between industries and topics (dimesions: #indutries x # topics)\n",
    "\n",
    "output\n",
    "list of probabilities for each topic\n",
    "'''\n",
    "def tpc2indstr_cluster(tpcs, indstr_tpc_matrix):\n",
    "    # turn topic probabilities into sparse row vector\n",
    "    val = []\n",
    "    col = []\n",
    "    for topic in tpcs:\n",
    "        col.append(topic[0]) # index of topic\n",
    "        val.append(topic[1]) # probability that industry belong to this topic\n",
    "    tpc_vector = sps.csc_matrix((np.array(val), (np.zeros(len(val)), np.array(col))), shape=(1, num_topics))\n",
    "    \n",
    "    # combine probabilities with relation between topics and industries\n",
    "    indstr = indstr_tpc_matrix * tpc_vector.transpose()\n",
    "    \n",
    "    # normalize such that sum = 1 -> interpretable as probability\n",
    "    indstr_norm = indstr / indstr.sum()\n",
    "    \n",
    "    # convert sparse matrix into array into flattened list\n",
    "    return indstr_norm.transpose().toarray().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 24 ms, total: 11.2 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# to avoid permission error when running get_document_topics on executor\n",
    "tpcs = {k: lda2.get_document_topics(v) for (k,v) in corpus.collect()}\n",
    "tpcs_rdd = sc.parallelize(tpcs.iteritems(), part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recall: corpval = corpus.values().collect()\n",
    "instrs = tpcs_rdd.mapValues(lambda v: tpc2indstr_cluster(v, indstr_tpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "\n",
    "Both the code for a single machine and the code for a cluster lead to the following output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('US06398796-20020604.XML',\n",
       "  [0.06057124709195175,\n",
       "   0.10573301575757516,\n",
       "   0.06966995883601858,\n",
       "   0.04885883488366097,\n",
       "   0.03686248434392848,\n",
       "   0.05246972371934844,\n",
       "   0.011626566913621985,\n",
       "   0.02329907329891271,\n",
       "   0.038881210540898416,\n",
       "   0.042260456674565385,\n",
       "   0.05192760959506576,\n",
       "   0.046482927690773265,\n",
       "   0.05991594511393557,\n",
       "   0.07001816033709758,\n",
       "   0.07116977112764734,\n",
       "   0.06602644997251943,\n",
       "   0.02306227708637173,\n",
       "   0.027036300864515205,\n",
       "   0.0727274742755223,\n",
       "   0.009109375931653937,\n",
       "   0.012291135944415972])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instrs.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Additional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to put our industry prediction into a larger context and also to validate them, we extract metadata from patents such as the inventor's geographic location, name, and the patent's grant date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supp = data_rdd.mapValues(lambda v: get_supplement_lxml(v, xml_format='pre2005'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joined = supp.join(instrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 s, sys: 8.63 s, total: 11.7 s\n",
      "Wall time: 12min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('US06659620-20031209.XML',\n",
       "  (['Saitama', None, 'JP', 'Jordan and Hamburg LLP'],\n",
       "   [0.08054396266495348,\n",
       "    0.08128860298136581,\n",
       "    0.023158182775195733,\n",
       "    0.01962892205371323,\n",
       "    0.07515835664634257,\n",
       "    0.07140715224128373,\n",
       "    0.019797497078430804,\n",
       "    0.006786676430313621,\n",
       "    0.032930273229061564,\n",
       "    0.02245926020931655,\n",
       "    0.10732746890213565,\n",
       "    0.03673797683032208,\n",
       "    0.0718654910679785,\n",
       "    0.047969165869005397,\n",
       "    0.08237789050249114,\n",
       "    0.08681317757547358,\n",
       "    0.051267234953028214,\n",
       "    0.03036526211576938,\n",
       "    0.01740802801738065,\n",
       "    0.014293692222322286,\n",
       "    0.020415725634116057]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "joined.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in1list(patent,data):\n",
    "    supp = data[0]\n",
    "    indstrs = data[1]\n",
    "    patent_id, date = patent[:-4].split('-')\n",
    "    merged = [patent_id, date] + supp + indstrs\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_flat = joined.map(lambda (k,v): in1list(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['US06659620',\n",
       "  '20031209',\n",
       "  'Saitama',\n",
       "  None,\n",
       "  'JP',\n",
       "  'Jordan and Hamburg LLP',\n",
       "  0.08054396266495348,\n",
       "  0.08128860298136581,\n",
       "  0.023158182775195733,\n",
       "  0.01962892205371323,\n",
       "  0.07515835664634257,\n",
       "  0.07140715224128373,\n",
       "  0.019797497078430804,\n",
       "  0.006786676430313621,\n",
       "  0.032930273229061564,\n",
       "  0.02245926020931655,\n",
       "  0.10732746890213565,\n",
       "  0.03673797683032208,\n",
       "  0.0718654910679785,\n",
       "  0.047969165869005397,\n",
       "  0.08237789050249114,\n",
       "  0.08681317757547358,\n",
       "  0.051267234953028214,\n",
       "  0.03036526211576938,\n",
       "  0.01740802801738065,\n",
       "  0.014293692222322286,\n",
       "  0.020415725634116057]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_flat.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "supplements = ['city', 'state', 'country', 'assignee']\n",
    "colnames = ['patent_id', 'date'] + supplements + industries\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(joined_flat, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336352 336352\n"
     ]
    }
   ],
   "source": [
    "print df.count(), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(patent_id=u'US06659620', date=u'20031209', city=u'Saitama', state=None, country=u'JP', assignee=u'Jordan and Hamburg LLP', 311=u'0.08054396266495348', 312=u'0.08128860298136581', 313=u'0.023158182775195733', 314=u'0.01962892205371323', 315=u'0.07515835664634257', 316=u'0.07140715224128373', 321=u'0.019797497078430804', 322=u'0.006786676430313621', 323=u'0.032930273229061564', 324=u'0.02245926020931655', 325=u'0.10732746890213565', 326=u'0.03673797683032208', 327=u'0.0718654910679785', 331=u'0.047969165869005397', 332=u'0.08237789050249114', 333=u'0.08681317757547358', 334=u'0.051267234953028214', 335=u'0.03036526211576938', 336=u'0.01740802801738065', 337=u'0.014293692222322286', 339=u'0.020415725634116057'),\n",
       " Row(patent_id=u'US06613991', date=u'20030902', city=u'Murfreesboro', state=u'TN', country=None, assignee=u'France/Scott Fetzer Company', 311=u'0.06978115088587478', 312=u'0.1677635784388767', 313=u'0.05630981385863626', 314=u'0.033616124725763585', 315=u'0.0', 316=u'0.0414703351793551', 321=u'0.010018528775422895', 322=u'0.005518261705159277', 323=u'0.010652039263906309', 324=u'0.02189757465040768', 325=u'0.20941370060986', 326=u'0.034970553209682016', 327=u'0.09484259687604234', 331=u'0.08559931534316671', 332=u'0.02273052278825903', 333=u'0.0375620106755801', 334=u'0.028496521338423373', 335=u'0.027367502522783308', 336=u'0.03693525320593837', 337=u'0.0', 339=u'0.0050546159468622814'),\n",
       " Row(patent_id=u'US06606177', date=u'20030812', city=u'Tokyo', state=None, country=u'JP', assignee=u'Hitachi, Ltd.', 311=u'0.06295879890071934', 312=u'0.02996376475126383', 313=u'0.04840206488798329', 314=u'0.048833003975022694', 315=u'0.04057064145497313', 316=u'0.05002315258876869', 321=u'0.011724883596627008', 322=u'0.05807344050148312', 323=u'0.05177533057422238', 324=u'0.03603657431623341', 325=u'0.15251518419582064', 326=u'0.05745284114163517', 327=u'0.04366483311371377', 331=u'0.06499775526746902', 332=u'0.05020728529837092', 333=u'0.06906229821286748', 334=u'0.06683840694859866', 335=u'0.004507036978540902', 336=u'0.011488765897333978', 337=u'0.02602107273824184', 339=u'0.014882864660110604')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Saving'></a>\n",
    "## Saving to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.repartition(1).write.save('s3n://cs109project/df11', format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='A'></a>\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A1'></a>\n",
    "### Spark Issues\n",
    "**Issue of ldamodel.get_document_topics with spark (YARN modus)**  \n",
    "Gensim imports theano. This fails because theano can't write in /home on executors\n",
    "\n",
    "*Solution Attempts*\n",
    "- Set permissions on executors in runtime (`os.system('sudo chmod -R ugo+rw /home')`)\n",
    "- Set environment variable either in bootstrap-actions (install-anaconda-emr): `export SPARK_YARN_USER_ENV=\"HOME=/tmp, THEANORC=/tmp\"` or in runtime (`os.environ['HOME'] = '/tmp'`)\n",
    "\n",
    "*Working Solution*\n",
    "- None\n",
    "\n",
    "<br>\n",
    "\n",
    "**Issue of ntlk with Spark (YARN modus)**  \n",
    "Spark's executors do not know where the manually downloaded packages of nltk are. \n",
    "\n",
    "Attempts\n",
    "Set environment variable \n",
    "- either in configure-spark.sh (`export SPARK_YARN_USER_ENV=\"NTLK_DATA=/usr/share/ntlk_data\"`)\n",
    "- or on executors in runtime (`os.environ['NLTK_DATA'] = '/usr/share/nltk_data'`)\n",
    "\n",
    "\n",
    "Working solution (but much slower than using pattern instead of nltk)\n",
    "\n",
    "Download packages during function where packages are needed.\n",
    "- new issue: permission denied to default folder\n",
    "- attempted solution: try to get read and write permissions for /usr via bash\n",
    "```\n",
    "    # try to get read and write permissions for /usr\n",
    "    bashCommand = 'sudo chmod -R ugo+rw /usr'\n",
    "    import subprocess\n",
    "    process = subprocess.call(bashCommand.split())\n",
    "    bashCommand = 'mkdir /usr/share/nltk_data'\n",
    "    process = subprocess.call(bashCommand.split())\n",
    "```\n",
    "\n",
    "- working solution: install packages in /tmp\n",
    "\n",
    "```\n",
    "def get_nouns(text):\n",
    "    if '/tmp' not in nltk.data.path:\n",
    "        nltk.data.path.append('/tmp')\n",
    "    if 'tokenizers' not in os.listdir('/tmp'):\n",
    "        nltk.download('punkt', '/tmp')\n",
    "        nltk.download('maxent_treebank_pos_tagger', '/tmp')\n",
    "        nltk.download('averaged_perceptron_tagger', '/tmp')\n",
    "    tokenized = word_tokenize(text.lower())\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    nouns = [a for (a, b) in tagged if b == 'NN']\n",
    "    return nouns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A2'></a>\n",
    "### Industry-Topic Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the output of the LDA analysis and maps it to the SIC definitions, which are scraped from the web (as indicated below). We chose not to pursue this approach further, because the NAICS Standard offers a much more finely grained classification system with richer definitions.\n",
    "\n",
    "First, we will walk through the steps to save the relevant data from the LDA model for further processing. This code is to be appended to the relevant parsing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set number of topics to 20 and the displayed words to 50. Output is formatted as a tuple (probability, word)\n",
    "lda2model = lda2.show_topics(num_topics=20, num_words=50, log=False, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save entire model as backup\n",
    "lda2.save('lda2model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save to json - our baseline format\n",
    "import json\n",
    "\n",
    "with open('lda2model.txt', 'w') as outfile:\n",
    "    json.dump(lda2.show_topics(num_topics=20, num_words=50, log=False, formatted=False), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the LDA model\n",
    "lda2_data = open('lda2model.txt')\n",
    "json_str = lda2_data.read()\n",
    "lda2_topics = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below maps topics to SIC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the LDA model\n",
    "lda2_data = open('lda2model.txt')\n",
    "json_str = lda2_data.read()\n",
    "lda2_topics = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract words (without probabilities) and zip in dict\n",
    "import time\n",
    "%time\n",
    "\n",
    "lda2_words = []\n",
    "lda2_prob = []\n",
    "for topic in lda2_topics:\n",
    "    words = []\n",
    "    prob = []\n",
    "    for word in topic:\n",
    "        words.append(word[1])\n",
    "        prob.append(word[0])\n",
    "    lda2_words.append(words)\n",
    "    lda2_prob.append(prob)\n",
    "\n",
    "topic_dict = dict(zip(xrange(len(lda2_words)), lda2_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the SIC Codes\n",
    "SIC_Data = open('SIC_Codes_Dict.txt')\n",
    "json_str = SIC_Data.read()\n",
    "SIC_Dict = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finding the intersection\n",
    "%time\n",
    "final_topic_dict = {}\n",
    "for topic in topic_dict:\n",
    "    counter = 0\n",
    "    tag = 0\n",
    "    for industry in SIC_Dict:\n",
    "        sect = set(topic_dict[topic]) & set(SIC_Dict[industry])\n",
    "        if len(sect) > counter:\n",
    "            counter = len(sect)\n",
    "            tag = industry\n",
    "    final_topic_dict[topic] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict is of form: key = lda topic no., value = SIC Major Group (see example below)\n",
    "final_topic_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](Data\\Images\\2.4 Example Intersection.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
