{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Processing and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Log\n",
    "### v.1\n",
    "- initial build\n",
    "\n",
    "### v.2 \n",
    "- added code for Spark\n",
    "\n",
    "### v.3 \n",
    "- added code to load data from web\n",
    "\n",
    "### v.4\n",
    "- switched from `ntlk` to `pattern` due to issues of `nltk` with Spark\n",
    "\n",
    "### v.5\n",
    "- switched from `BeautifulSoup` to `lxml` for performance\n",
    "- mapping from industries to topics\n",
    "- saves results to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not necessary on AWS\n",
    "! pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "#import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from pattern.en import tag\n",
    "import collections\n",
    "import gensim\n",
    "import requests, zipfile, StringIO\n",
    "from lxml import etree \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when using (not necessary on AWS)\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('maxent_treebank_pos_tagger')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark setup for vagrant (not necessary on AWS)\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "# Depending on your setup you might have to change this line of code\n",
    "#findspark makes sure I dont need the below on homebrew.\n",
    "#os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1/libexec/\"\n",
    "#the below actually broke my spark, so I removed it. \n",
    "#Depending on how you started the notebook, you might need it.\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local pyspark --executor-memory 4g\"\n",
    "\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set('spark.executor.memory', '4g')\n",
    "    )\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# load data from web (given url to file with urls of zips)\n",
    "# e.g.'https://s3.amazonaws.com/cs109project/2004.txt')\n",
    "# for each zip: loads all xmls files into dictionary with key=filename\n",
    "\n",
    "urls = 'https://s3.amazonaws.com/cs109project/2002-2004.txt'\n",
    "rs = requests.get(urls)\n",
    "data = {}\n",
    "for url in rs.content.split('\\r\\n'):\n",
    "    #print url\n",
    "    r = requests.get(url)\n",
    "    if zipfile.is_zipfile(StringIO.StringIO(r.content)):\n",
    "        z = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "        xmls = [member for member in z.namelist() if os.path.splitext(member.lower())[1]=='.xml']\n",
    "        newdata = {os.path.basename(xml): z.open(xml).read() for xml in xmls}\n",
    "        data.update(newdata)\n",
    "    else:\n",
    "        print url+' contains no zip file.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 2.18 s, total: 4.85 s\n",
      "Wall time: 5.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data from web (given url to zip file)\n",
    "# e.g. e.g.'https://s3.amazonaws.com/cs109project/Unpacked+Data.zip'\n",
    "# loads all xmls files from given zip into dictionary with key=filename\n",
    "\n",
    "url = 'https://s3.amazonaws.com/cs109project/Unpacked+Data.zip'\n",
    "r = requests.get(url)\n",
    "if zipfile.is_zipfile(StringIO.StringIO(r.content)):\n",
    "    z = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "    xmls = [member for member in z.namelist()[:100] if os.path.splitext(member.lower())[1]=='.xml']\n",
    "    data = {os.path.basename(xml): z.open(xml).read() for xml in xmls}\n",
    "else:\n",
    "    print 'URL does not link to zip file. Please provide valid URL.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from disk\n",
    "# loads data with structure year\\weeks\\xmls into dictionary with key=filename\n",
    "source = os.getcwd()\n",
    "path = os.path.join(source,'2014')\n",
    "data = {}\n",
    "for week in os.listdir(path):\n",
    "    week_path = os.path.join(path, week)\n",
    "    for patent in os.listdir(week_path):\n",
    "        patent_path = os.path.join(week_path, patent)\n",
    "        if os.path.isfile(patent_path):\n",
    "            with open(patent_path, 'r') as my_file:\n",
    "                data[patent] = my_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500642"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_abstract_soup(xml, xml_format='pre2005'):\n",
    "    if xml_format=='pre2005':\n",
    "        abstract = BeautifulSoup(xml, 'lxml').find('sdoab').get_text().strip()\n",
    "    else:\n",
    "        abstract = BeautifulSoup(xml, 'lxml').find('abstract').get_text()\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_abstract_lxml(xml, xml_format='pre2005'):\n",
    "    if xml_format=='pre2005':\n",
    "        xml = xml.replace('&','')\n",
    "        abstract = etree.XML(xml).xpath('//SDOAB//PDAT')[0].text\n",
    "    else:\n",
    "        abstract = etree.XML(xml).xpath('//abstract')[0][0].text\n",
    "        \n",
    "    if not abstract: #lxml less robust than soup (e.g. lxml returns None for US08623623-20140107.XML)\n",
    "        abstract = get_abstract_soup(xml, xml_format)\n",
    "        \n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_supplement_lxml(xml, xml_format='pre2005'):\n",
    "    if xml_format=='pre2005':\n",
    "        xml = xml.replace('&','')\n",
    "        city_tag = '//CITY//PDAT'\n",
    "        state_tag = '//STATE//PDAT'\n",
    "        assignee_tag = '//ONM//PDAT'\n",
    "    else:\n",
    "        city_tag = '//city'\n",
    "        state_tag = '//state'\n",
    "        assignee_tag = '//assignee//orgname'        \n",
    "        \n",
    "    # first inventor geography \n",
    "    try:\n",
    "        inv_city = etree.XML(xml).xpath(city_tag)[0].text\n",
    "    except:\n",
    "        inv_city = None\n",
    "\n",
    "    try:\n",
    "        inv_state = etree.XML(xml).xpath(state_tag)[0].text\n",
    "    except:\n",
    "        inv_state = None\n",
    "\n",
    "    if inv_state:\n",
    "        try:\n",
    "            if xml_format=='pre2005':\n",
    "                inv_ctry = etree.XML(xml).xpath(state_tag)[0].getnext().getchildren()[0].text\n",
    "            else:\n",
    "                inv_ctry = etree.XML(xml).xpath(state_tag)[0].getnext().text\n",
    "        except:\n",
    "            inv_ctry = None\n",
    "    else:\n",
    "        try:\n",
    "            if xml_format=='pre2005':\n",
    "                inv_ctry = etree.XML(xml).xpath('//CITY')[0].getnext().getchildren()[0].text\n",
    "            else:\n",
    "                inv_ctry = etree.XML(xml).xpath(city_tag)[0].getnext().text            \n",
    "        except:\n",
    "            inv_ctry = None\n",
    "\n",
    "    # last org name        \n",
    "    try:\n",
    "        assignee = etree.XML(xml).xpath(assignee_tag)[0].text\n",
    "    except:\n",
    "        assignee = None\n",
    "    return [inv_city, inv_state, inv_ctry, assignee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wyomissing Hills', 'PA', None, 'Lucent Technologies Inc.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_supplement_lxml(data[data.keys()[1]])#, xml_format='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A self-aligned clock recovery circuit for synchronizing a local clock with an input data signal includes a sampling type phase detector for generating an output signal based on the phase difference between the local clock and the data signal timing. The phase detector obtains samples of consecutive data symbols at sampling times corresponding to transitions of the local clock, and obtains a data crossover sample at a sampling instant in between those of the consecutive data symbol samples. A phase shifter is employed to phase shift the local clock by an amount corresponding to a time varying modulation signal so as to obtain each data crossover sample at a variable sampling instant relative to the associated consecutive symbol samples. Logic circuitry determines whether the local clock appears to be early or late based on a comparison of the logic levels of the symbol samples and the associated data crossover sample, and provides a corresponding output signal through a filter to the local clock to adjust the clock accordingly. Since the relative sampling instants of successive data crossover samples are varied with time, the phase detector output signal amplitude is substantially proportional to the amount of phase error between the local clock and the symbol timing, thereby improving jitter properties of the clock recovery circuit.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abstract_lxml(data[data.keys()[1]])#, xml_format='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'A self-aligned clock recovery circuit for synchronizing a local clock with an input data signal includes a sampling type phase detector for generating an output signal based on the phase difference between the local clock and the data signal timing. The phase detector obtains samples of consecutive data symbols at sampling times corresponding to transitions of the local clock, and obtains a data crossover sample at a sampling instant in between those of the consecutive data symbol samples. A phase shifter is employed to phase shift the local clock by an amount corresponding to a time varying modulation signal so as to obtain each data crossover sample at a variable sampling instant relative to the associated consecutive symbol samples. Logic circuitry determines whether the local clock appears to be early or late based on a comparison of the logic levels of the symbol samples and the associated data crossover sample, and provides a corresponding output signal through a filter to the local clock to adjust the clock accordingly. Since the relative sampling instants of successive data crossover samples are varied with time, the phase detector output signal amplitude is substantially proportional to the amount of phase error between the local clock and the symbol timing, thereby improving jitter properties of the clock recovery circuit.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abstract_soup(data[data.keys()[1]])#, xml_format='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nouns(text):  \n",
    "    # using pattern\n",
    "    tagged = tag(text.lower(), tokenize=True)\n",
    "    \n",
    "    # using nltk (if statements only necessary on AWS)\n",
    "    #if '/tmp' not in nltk.data.path:\n",
    "    #    nltk.data.path.append('/tmp')\n",
    "    #if 'tokenizers' not in os.listdir('/tmp'):\n",
    "    #    nltk.download('punkt', '/tmp')\n",
    "    #    nltk.download('maxent_treebank_pos_tagger', '/tmp')\n",
    "    #    nltk.download('averaged_perceptron_tagger', '/tmp')\n",
    "    #tokenized = word_tokenize(text.lower())\n",
    "    #tagged = nltk.pos_tag(tokenized)\n",
    "    \n",
    "    nouns = [a for (a, b) in tagged if b == 'NN']\n",
    "    return nouns\n",
    "\n",
    "# for each noun in list of nouns: get its number and number of occurences in list\n",
    "def tocorpus(nouns,vocabulary):\n",
    "    count = collections.defaultdict(int) # to count number of occurences of a noun\n",
    "    for noun in nouns:\n",
    "        count[vocabulary[noun]] +=1  # for new nouns: creates new key, sets value to 1. for existing keys: increases value by 1\n",
    "    return count.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.42 s, sys: 36 ms, total: 4.46 s\n",
      "Wall time: 4.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# code does not require Spark\n",
    "\n",
    "data_parsed = {k: get_abstract_lxml(v) for (k,v) in data.iteritems()}\n",
    "data_nouns = {k: get_nouns(v) for (k,v) in data_parsed.iteritems()}\n",
    "\n",
    "# loops through all list of nouns and gets all nouns and creates set of them (set ->each noun only once)\n",
    "flat = {item for sublist in data_nouns.values() for item in sublist}\n",
    "\n",
    "vocab = dict(zip(flat,range(len(flat))))    #word-to-number dict \n",
    "id2word = dict(zip(range(len(flat)),flat))  #number-to-word dict\n",
    "\n",
    "corpus = {k: tocorpus(v,vocab) for (k,v) in data_nouns.iteritems()}\n",
    "corpval = corpus.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem        Inodes  IUsed     IFree IUse% Mounted on\r\n",
      "/dev/xvda1        655360 178231    477129   28% /\r\n",
      "devtmpfs        31485576    669  31484907    1% /dev\r\n",
      "tmpfs           31487830      1  31487829    1% /dev/shm\r\n",
      "/dev/xvdb      314572800   9190 314563610    1% /mnt\r\n",
      "/dev/xvdc      314572800     75 314572725    1% /mnt1\r\n"
     ]
    }
   ],
   "source": [
    "# on AWS/vagrant: check inodes\n",
    "! df -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "/dev/xvda1      9.8G  8.1G  1.7G  84% /\r\n",
      "devtmpfs        121G   68K  121G   1% /dev\r\n",
      "tmpfs           121G     0  121G   0% /dev/shm\r\n",
      "/dev/xvdb       300G  228M  300G   1% /mnt\r\n",
      "/dev/xvdc       300G  341M  300G   1% /mnt1\r\n"
     ]
    }
   ],
   "source": [
    "# on AWS/vagrant: check disk space\n",
    "! df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on AWS/vagrant: check memory usage\n",
    "! free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of partitions for rdd throughout notebook\n",
    "part = 16384#8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# code requires Spark\n",
    "\n",
    "data_rdd = sc.parallelize(data.iteritems(),part)\n",
    "print data_rdd.getNumPartitions()\n",
    "\n",
    "data_nouns = (data_rdd.mapValues(lambda v: get_abstract_lxml(v))\n",
    "               .mapValues(lambda v: get_nouns(v))\n",
    ").cache()\n",
    "\n",
    "# associates all distinct words with a number\n",
    "vocabtups = (data_nouns.flatMap(lambda (k,v): v)\n",
    "             .distinct()\n",
    "             .zipWithIndex()\n",
    ")\n",
    "\n",
    "vocab = vocabtups.collectAsMap()                            #word-to-number dict\n",
    "id2word = vocabtups.map(lambda (x,y): (y,x)).collectAsMap() #number-to-word dict\n",
    "\n",
    "corpus = data_nouns.mapValues(lambda v: tocorpus(v, vocab))\n",
    "corpval = corpus.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19627 19627\n",
      "19324\n"
     ]
    }
   ],
   "source": [
    "print len(data),len(corpval)\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.9 s, sys: 0 ns, total: 34.9 s\n",
      "Wall time: 34.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_topics=10\n",
    "lda2 = gensim.models.ldamodel.LdaModel(corpval, id2word=id2word, num_topics=num_topics, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.019*information + 0.017*method + 0.016*system + 0.015*invention + 0.015*network + 0.015*device + 0.014*interface + 0.013*user + 0.010*telephone + 0.009*tissue',\n",
       " u'0.041*portion + 0.025*end + 0.023*vehicle + 0.019*side + 0.016*device + 0.016*body + 0.015*wall + 0.015*container + 0.013*system + 0.013*panel',\n",
       " u'0.044*system + 0.021*unit + 0.019*method + 0.019*information + 0.015*control + 0.015*apparatus + 0.013*data + 0.012*computer + 0.011*communication + 0.011*ink',\n",
       " u'0.066*layer + 0.042*circuit + 0.031*substrate + 0.029*voltage + 0.025*semiconductor + 0.023*power + 0.016*electrode + 0.015*device + 0.015*gate + 0.014*film',\n",
       " u'0.031*device + 0.030*position + 0.022*frame + 0.017*beam + 0.017*assembly + 0.015*lens + 0.015*end + 0.015*direction + 0.014*axis + 0.013*plate',\n",
       " u'0.023*filter + 0.018*air + 0.016*surface + 0.016*water + 0.013*toner + 0.013*ring + 0.013*element + 0.012*vacuum + 0.010*roller + 0.010*pipe',\n",
       " u'0.043*material + 0.023*metal + 0.020*surface + 0.016*heat + 0.013*coating + 0.013*method + 0.013*resin + 0.012*sheet + 0.011*process + 0.011*invention',\n",
       " u'0.061*member + 0.026*end + 0.023*housing + 0.022*support + 0.022*portion + 0.021*valve + 0.018*surface + 0.012*body + 0.011*assembly + 0.010*air',\n",
       " u'0.053*signal + 0.036*image + 0.026*memory + 0.022*output + 0.020*input + 0.019*device + 0.015*data + 0.012*display + 0.012*circuit + 0.012*line',\n",
       " u'0.035*invention + 0.024*gas + 0.020*method + 0.018*engine + 0.014*composition + 0.013*temperature + 0.012*acid + 0.012*pressure + 0.012*process + 0.011*agent']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch naics nouns\n",
    "naics_url = 'https://s3.amazonaws.com/cs109projectr/naics_nouns.json'\n",
    "naics_r = requests.get(naics_url)\n",
    "naics_nouns = json.loads(naics_r.content)\n",
    "\n",
    "# take only manufacturing\n",
    "industries =  [u'311',u'312',u'313',u'314',u'315',u'316',u'321',u'322',u'323',u'324',u'325',u'326',u'327',u'331',u'332',u'333',u'334',u'335',u'336',u'337',u'339',\n",
    "              ]\n",
    "naics_subset = []\n",
    "for i in naics_nouns:\n",
    "    if i['naics_code'] in industries:\n",
    "        naics_subset.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 26\n"
     ]
    }
   ],
   "source": [
    "# check that no invalid naics code entered\n",
    "print len(industries), len(naics_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format naics data\n",
    "'''\n",
    "format_naics\n",
    "input = list of dictionaries with naics definitions (json output of ind definitions ipython notebook)\n",
    "output = naics_tuples, naics_codes\n",
    "naics_tuples = list of tuples, tuple1 = (noun1, count1), tuple2 = ()...\n",
    "naics_codes = list of naics codes\n",
    "'''\n",
    "def format_naics(input):\n",
    "    naics_tuples = [i['noun_dict'].items() for i in input]\n",
    "    naics_codes = [i['naics_code'] for i in input]\n",
    "    return naics_tuples, naics_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 26\n"
     ]
    }
   ],
   "source": [
    "naics_tuples, naics_codes = format_naics(naics_subset)\n",
    "print len(naics_tuples), len(naics_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of nouns in naics definitions that do not appear in patents\n"
     ]
    }
   ],
   "source": [
    "# replace naics nouns with vocab id\n",
    "# i.e. preps naics definitions with the same signature as corpval\n",
    "print 'list of nouns in naics definitions that do not appear in patents'\n",
    "naics_output = []\n",
    "for i in naics_tuples:\n",
    "    new_noun_list = []\n",
    "    for j in i:\n",
    "        try:\n",
    "            new_noun_list.append((vocab[j[0]], j[1]))\n",
    "        except:\n",
    "            pass\n",
    "            #print j[0]\n",
    "    naics_output.append(new_noun_list)\n",
    "\n",
    "# consider dropping industries with fewer than 10 nouns or whatever cutoff seems appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sparse matrix of industry-topic relation\n",
    "val = []\n",
    "col = []\n",
    "row = []\n",
    "for i,industry in enumerate(naics_output):\n",
    "    for topic in lda2.get_document_topics(industry):\n",
    "        row.append(i) # only changes after len(industry) numbers of loops\n",
    "        col.append(topic[0]) # index of topic\n",
    "        val.append(topic[1]) # probability that industry belong to this topic\n",
    "# spark doc recommends np.array over list for efficiency (http://spark.apache.org/docs/latest/mllib-data-types.html)\n",
    "indstr_tpc = sps.csc_matrix((np.array(val), (np.array(row), np.array(col))), shape=(len(industries), num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1232419 ,  0.02598243,  0.34301399,  0.18799013,  0.03144995,\n",
       "         0.06063741,  0.07118193,  0.01069146,  0.04197328,  0.10383751],\n",
       "       [ 0.08276814,  0.38395326,  0.04007173,  0.06566465,  0.04191848,\n",
       "         0.03061305,  0.08926058,  0.04507391,  0.08268692,  0.13798929],\n",
       "       [ 0.02378697,  0.09066628,  0.02678708,  0.02038845,  0.04258457,\n",
       "         0.10494352,  0.45217201,  0.07185087,  0.        ,  0.16540534],\n",
       "       [ 0.01729065,  0.12850362,  0.05333094,  0.0305271 ,  0.09097744,\n",
       "         0.04485774,  0.44873224,  0.08416759,  0.        ,  0.10141961],\n",
       "       [ 0.03078105,  0.07158176,  0.02249734,  0.        ,  0.        ,\n",
       "         0.17695425,  0.35215268,  0.        ,  0.13922742,  0.20511032],\n",
       "       [ 0.07686529,  0.0344051 ,  0.0631793 ,  0.1752406 ,  0.0279589 ,\n",
       "         0.06712524,  0.12061129,  0.05991341,  0.0819431 ,  0.29275777],\n",
       "       [ 0.        ,  0.0285028 ,  0.34632276,  0.        ,  0.02232747,\n",
       "         0.09553749,  0.35977942,  0.10439474,  0.        ,  0.03538187],\n",
       "       [ 0.06599222,  0.        ,  0.09976147,  0.        ,  0.37269892,\n",
       "         0.08373823,  0.33607742,  0.0165224 ,  0.        ,  0.01374548],\n",
       "       [ 0.04775218,  0.10075932,  0.1174244 ,  0.01615645,  0.06420148,\n",
       "         0.070417  ,  0.45083001,  0.05828918,  0.01794761,  0.05622238],\n",
       "       [ 0.13946375,  0.01352056,  0.59274811,  0.        ,  0.02985473,\n",
       "         0.11487449,  0.        ,  0.02241086,  0.08207153,  0.        ],\n",
       "       [ 0.06347434,  0.01259741,  0.28061337,  0.        ,  0.07640328,\n",
       "         0.02450185,  0.07604533,  0.        ,  0.41655164,  0.04941743],\n",
       "       [ 0.08222831,  0.        ,  0.77089412,  0.        ,  0.01017411,\n",
       "         0.0231773 ,  0.        ,  0.09132641,  0.02026704,  0.        ],\n",
       "       [ 0.16472918,  0.        ,  0.66525763,  0.01280967,  0.        ,\n",
       "         0.01297868,  0.        ,  0.08856877,  0.03990584,  0.01447863],\n",
       "       [ 0.11120634,  0.        ,  0.63425825,  0.        ,  0.05727574,\n",
       "         0.08504398,  0.        ,  0.01674914,  0.08894241,  0.        ],\n",
       "       [ 0.09182287,  0.10721263,  0.08179679,  0.09770947,  0.08884975,\n",
       "         0.11376875,  0.2872421 ,  0.05800324,  0.01221886,  0.06137555],\n",
       "       [ 0.05875137,  0.04842503,  0.18025925,  0.1666221 ,  0.03781559,\n",
       "         0.11849166,  0.23508154,  0.04591907,  0.01149762,  0.09713677],\n",
       "       [ 0.07603275,  0.04394207,  0.13781459,  0.01531504,  0.0167989 ,\n",
       "         0.23868588,  0.42347429,  0.03240713,  0.01311422,  0.        ],\n",
       "       [ 0.07379039,  0.06595445,  0.06905888,  0.08538443,  0.06454431,\n",
       "         0.06150705,  0.40450939,  0.02880688,  0.01871992,  0.1277243 ],\n",
       "       [ 0.03645254,  0.14298166,  0.03555697,  0.05251731,  0.04864623,\n",
       "         0.0984374 ,  0.49418846,  0.04152439,  0.        ,  0.04255456],\n",
       "       [ 0.03659552,  0.1797992 ,  0.12738645,  0.02171058,  0.17451206,\n",
       "         0.04729795,  0.36330412,  0.        ,  0.0163865 ,  0.02631034],\n",
       "       [ 0.10293975,  0.        ,  0.56229241,  0.        ,  0.02334816,\n",
       "         0.06162978,  0.10064041,  0.04709938,  0.04284304,  0.04667053],\n",
       "       [ 0.01125067,  0.02068785,  0.        ,  0.02333575,  0.06490791,\n",
       "         0.509627  ,  0.28708193,  0.02686409,  0.        ,  0.0415903 ],\n",
       "       [ 0.        ,  0.10158944,  0.01167397,  0.        ,  0.02075811,\n",
       "         0.07826701,  0.55840724,  0.02254628,  0.        ,  0.18572137],\n",
       "       [ 0.06667541,  0.03749992,  0.        ,  0.11307795,  0.02943505,\n",
       "         0.10049558,  0.437616  ,  0.03485151,  0.01287184,  0.1591698 ],\n",
       "       [ 0.02028102,  0.        ,  0.0509645 ,  0.        ,  0.        ,\n",
       "         0.08954931,  0.12305241,  0.0166749 ,  0.        ,  0.68653981],\n",
       "       [ 0.07419457,  0.02734742,  0.02457786,  0.10383183,  0.01175345,\n",
       "         0.10610879,  0.43008464,  0.01993602,  0.01107568,  0.19108974]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indstr_tpc.toarray() # sanity check: compare this to [lda2.get_document_topics(i) for i in naics_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input\n",
    "nouns: list of nouns with signature as corpval\n",
    "lda: trained lda classifier\n",
    "indstr_tpc_matrix: matrix of relation between industries and topics (dimesions: #indutries x # topics)\n",
    "\n",
    "output\n",
    "list of probabilities for each topic\n",
    "'''\n",
    "def tpc2indstr(nouns, lda, indstr_tpc_matrix):\n",
    "    topics = lda.get_document_topics(nouns)\n",
    "    #os.environ['HOME'] = '/tmp'\n",
    "    # turn topic probabilities into sparse row vector\n",
    "    val = []\n",
    "    col = []\n",
    "    for topic in topics:\n",
    "        col.append(topic[0]) # index of topic\n",
    "        val.append(topic[1]) # probability that industry belong to this topic\n",
    "    tpc_vector = sps.csc_matrix((np.array(val), (np.zeros(len(val)), np.array(col))), shape=(1, num_topics))\n",
    "    \n",
    "    # combine probabilities with relation between topics and industries\n",
    "    indstr = indstr_tpc_matrix * tpc_vector.transpose()\n",
    "    \n",
    "    # normalize such that sum = 1 -> interpretable as probability\n",
    "    indstr_norm = indstr / indstr.sum()\n",
    "    \n",
    "    # convert sparse matrix into array into flattened list\n",
    "    return indstr_norm.transpose().toarray().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on single instance: grant permission\n",
    "! sudo chmod -R ugo+rw /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WORKS ONLY ON SINGLE INSTANCE,NOT ON CLUSTER\n",
    "# recall: corpval = corpus.values().collect()\n",
    "instrs = corpus.mapValues(lambda v: tpc2indstr(v, lda2, indstr_tpc))\n",
    "supp = data_rdd.mapValues(lambda v: get_supplement_lxml(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input\n",
    "nouns: list of nouns with signature as corpval\n",
    "lda: trained lda classifier\n",
    "indstr_tpc_matrix: matrix of relation between industries and topics (dimesions: #indutries x # topics)\n",
    "\n",
    "output\n",
    "list of probabilities for each topic\n",
    "'''\n",
    "def tpc2indstr_cluster(tpcs, indstr_tpc_matrix):\n",
    "    # turn topic probabilities into sparse row vector\n",
    "    val = []\n",
    "    col = []\n",
    "    for topic in tpcs:\n",
    "        col.append(topic[0]) # index of topic\n",
    "        val.append(topic[1]) # probability that industry belong to this topic\n",
    "    tpc_vector = sps.csc_matrix((np.array(val), (np.zeros(len(val)), np.array(col))), shape=(1, num_topics))\n",
    "    \n",
    "    # combine probabilities with relation between topics and industries\n",
    "    indstr = indstr_tpc_matrix * tpc_vector.transpose()\n",
    "    \n",
    "    # normalize such that sum = 1 -> interpretable as probability\n",
    "    indstr_norm = indstr / indstr.sum()\n",
    "    \n",
    "    # convert sparse matrix into array into flattened list\n",
    "    return indstr_norm.transpose().toarray().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 24 ms, total: 11.2 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# to avoid permission error when running get_document_topics on executor\n",
    "tpcs = {k: lda2.get_document_topics(v) for (k,v) in corpus.collect()}\n",
    "tpcs_rdd = sc.parallelize(tpcs.iteritems(), part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recall: corpval = corpus.values().collect()\n",
    "instrs = tpcs_rdd.mapValues(lambda v: tpc2indstr_cluster(v, indstr_tpc))\n",
    "supp = data_rdd.mapValues(lambda v: get_supplement_lxml(v, xml_format='pre2005'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same code for single instance and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('US06353594-20020305.XML',\n",
       "  [0.0909609107674464,\n",
       "   0.0289102386472117,\n",
       "   0.010148724715883515,\n",
       "   0.014195538238002063,\n",
       "   0.0060817315097496764,\n",
       "   0.05475341408531665,\n",
       "   0.03410002146605919,\n",
       "   0.018112463022192497,\n",
       "   0.021144688901333638,\n",
       "   0.07588262096461587,\n",
       "   0.03560342213243369,\n",
       "   0.08623378335443734,\n",
       "   0.08903768555858754,\n",
       "   0.07642027416733223,\n",
       "   0.04126517327215404,\n",
       "   0.0620940713088097,\n",
       "   0.026518177240714244,\n",
       "   0.03501149381947394,\n",
       "   0.019731012482289142,\n",
       "   0.02195631881617554,\n",
       "   0.06829588270496435,\n",
       "   0.00659029170498877,\n",
       "   0.0011494551266106208,\n",
       "   0.03346179536308506,\n",
       "   0.00756573119056009,\n",
       "   0.03477507943957262])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instrs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joined = supp.join(instrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 272 ms, total: 1.32 s\n",
      "Wall time: 1min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('US06341454-20020129.XML',\n",
       "  (['Montauk', 'NY', None, None],\n",
       "   [0.023133051577601534,\n",
       "    0.016631934939355518,\n",
       "    0.04682354118291602,\n",
       "    0.05067104242127523,\n",
       "    0.03577181775435819,\n",
       "    0.01946119564580897,\n",
       "    0.04457612544452538,\n",
       "    0.07805857604535972,\n",
       "    0.05060315743431658,\n",
       "    0.026108277011777332,\n",
       "    0.024415389196234257,\n",
       "    0.024272697117669283,\n",
       "    0.021915368415286438,\n",
       "    0.02853778572702099,\n",
       "    0.0423785116665556,\n",
       "    0.033931200898239026,\n",
       "    0.05004234027336171,\n",
       "    0.045677721097780545,\n",
       "    0.05074159257613148,\n",
       "    0.05515080886857469,\n",
       "    0.029974158257698377,\n",
       "    0.05026912378221485,\n",
       "    0.0500737438498226,\n",
       "    0.043805444947937054,\n",
       "    0.015048984897513107,\n",
       "    0.041926408970665525]))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "joined.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in1list(patent,data):\n",
    "    supp = data[0]\n",
    "    indstrs = data[1]\n",
    "    patent_id, date = patent[:-4].split('-')\n",
    "    merged = [patent_id, date] + supp + indstrs\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_flat = joined.map(lambda (k,v): in1list(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['US06341454',\n",
       "  '20020129',\n",
       "  'Montauk',\n",
       "  'NY',\n",
       "  None,\n",
       "  None,\n",
       "  0.023133051577601534,\n",
       "  0.016631934939355518,\n",
       "  0.04682354118291602,\n",
       "  0.05067104242127523,\n",
       "  0.03577181775435819,\n",
       "  0.01946119564580897,\n",
       "  0.04457612544452538,\n",
       "  0.07805857604535972,\n",
       "  0.05060315743431658,\n",
       "  0.026108277011777332,\n",
       "  0.024415389196234257,\n",
       "  0.024272697117669283,\n",
       "  0.021915368415286438,\n",
       "  0.02853778572702099,\n",
       "  0.0423785116665556,\n",
       "  0.033931200898239026,\n",
       "  0.05004234027336171,\n",
       "  0.045677721097780545,\n",
       "  0.05074159257613148,\n",
       "  0.05515080886857469,\n",
       "  0.029974158257698377,\n",
       "  0.05026912378221485,\n",
       "  0.0500737438498226,\n",
       "  0.043805444947937054,\n",
       "  0.015048984897513107,\n",
       "  0.041926408970665525]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_flat.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "supplements = ['city', 'state', 'country', 'assignee']\n",
    "colnames = ['patent_id', 'date'] + supplements + industries\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(joined_flat, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19627 19627\n"
     ]
    }
   ],
   "source": [
    "print df.count(), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(patent_id=u'US06341454', date=u'20020129', city=u'Montauk', state=u'NY', country=None, assignee=None, 311=u'0.023133051577601534', 312=u'0.016631934939355518', 313=u'0.04682354118291602', 314=u'0.05067104242127523', 315=u'0.03577181775435819', 316=u'0.01946119564580897', 321=u'0.04457612544452538', 322=u'0.07805857604535972', 323=u'0.05060315743431658', 324=u'0.026108277011777332', 325=u'0.024415389196234257', 326=u'0.024272697117669283', 327=u'0.021915368415286438', 331=u'0.02853778572702099', 332=u'0.0423785116665556', 333=u'0.033931200898239026', 334=u'0.05004234027336171', 335=u'0.045677721097780545', 336=u'0.05074159257613148', 337=u'0.05515080886857469', 339=u'0.029974158257698377', 511=u'0.05026912378221485', 512=u'0.0500737438498226', 515=u'0.043805444947937054', 516=u'0.015048984897513107', 517=u'0.041926408970665525'),\n",
       " Row(patent_id=u'US06350229', date=u'20020226', city=u'Bilthoven', state=u'MN', country=None, assignee=u'Medtronic, Inc.', 311=u'0.024644634320671217', 312=u'0.18370716322208444', 313=u'0.049352535816755465', 314=u'0.06300612051695463', 315=u'0.04363077112615755', 316=u'0.031080100258748248', 321=u'0.015478564263209895', 322=u'0.006459251794426127', 323=u'0.05168018896900305', 324=u'0.01784935568002878', 325=u'0.012105587782336549', 326=u'0.006213022666767573', 327=u'0.012354550950554101', 331=u'0.009267320985211614', 332=u'0.058576188788586954', 333=u'0.030906292546849307', 334=u'0.029051899484232632', 335=u'0.03990218079418686', 336=u'0.06994040665366372', 337=u'0.08522259803622496', 339=u'0.009742865303257672', 511=u'0.01961479089006702', 512=u'0.05278428457995976', 515=u'0.02816798879997312', 516=u'0.024060849734228784', 517=u'0.025200486035859847'),\n",
       " Row(patent_id=u'US06353167', date=u'20020305', city=u'Colorado Springs', state=u'CO', country=None, assignee=u'Raglan Productions, Inc.', 311=u'0.06021679015765346', 312=u'0.01710456045611624', 313=u'0.006880753912373329', 314=u'0.01121704843045417', 315=u'0.014160602552384778', 316=u'0.0161220518817926', 321=u'0.05890042702270942', 322=u'0.01743303986850949', 323=u'0.022682170830611855', 324=u'0.10421571732914148', 325=u'0.06951941761479283', 326=u'0.12928741673358862', 327=u'0.11270855521823198', 331=u'0.11096534645359805', 332=u'0.01700757469282504', 333=u'0.03249275427003557', 334=u'0.026724984649404846', 335=u'0.014089544808237854', 336=u'0.009040517584625905', 337=u'0.02517632877057003', 339=u'0.09628575094966794', 511=u'0.005630345325297354', 512=u'0.004254812050614783', 515=u'0.0022988797832454257', 516=u'0.00939460129121969', 517=u'0.00619000736229733')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.repartition(1).write.save('s3n://cs109project/df8', format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
