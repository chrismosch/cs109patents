{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Unpacking Google Patent Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Constantin Knoll, Christopher Mosch, Rohan Thavarajah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to automate the downloading, unpacking, and subsequent uploading (to S3) of patent data available on Google. We scrape patent files with supplements and images from the url found in the User Guide. The data is grouped by weeks, ranging from 2001 to 2015. The years 2011- are compressed in .tar format, while the earlier ones are compressed as .zip. Thus, the code accounts for these options by including the python libraries ZipFile and TarFile. \n",
    "The code found in Compact Version (which we used mainly) downloads a single week of patent data, extracts, uploads and deletes it before repeating the process with another week. This is optimized to work on very small storage space such as an SSD on our local machine.\n",
    "Each week of patent data contains several folders with different content. For the purposes of our project, we are only interested in the abstracts, which are found in the .xml files of the patent application body. Thus, we need to get rid of all supplements (as defined by the user below) and all images that aren't conducive to data analysis. This is reflected in the main Code, where the tree of the downloaded data is searched for all unwanted folders, which are deleted.\n",
    "It is important to realize that the \"deleted\" files are typically dropped into the recycle bin of the local machine, and thus the purpose of running the space-optimized code is compromised. Therefore we advise the user to us an automated recycle bin clean-up program. It should be set to clean every 10-15 minutes, which is the average time it takes to complete the cycle for one week of patent data.\n",
    "Since we get rid of most of the data found in the patent files, the convoluted folder structure that remains is thrown out and replaced by a three-tiered one: 1) Root 2) Year 3) Week.\n",
    "The uploading to S3 is controlled by the boto3 library, which takes care of most of the uploading automatically.\n",
    "\n",
    "![Image](Data/Images/Workflow_2.1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href='#Change Log'>Change Log</a>\n",
    "    * <a href='#v1'>v1</a>\n",
    "    * <a href='#v2'>v2</a>\n",
    "    * <a href='#v3'>v3</a>\n",
    "* <a href='#User Guide'>User Guide</a>\n",
    "    * <a href='#Path Specification'>Path Specification</a>\n",
    "* <a href='#Main Code'>Main Code</a>\n",
    "    * <a href='#Extraction'>Extraction</a>\n",
    "    * <a href='#Cleaning'>Cleaning</a>\n",
    "    * <a href='#Uploading'>Uploading</a>\n",
    "* <a href='#Compact Version'>Compact Version</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Change Log'></a>\n",
    "## Change Log / Notes\n",
    "\n",
    "<a id='v1'></a>\n",
    "### v.1\n",
    "- initial build\n",
    "- only supports 'keep_supplements == False, etc.'\n",
    "- unpacking can't be paused and resumed - only entire .tar files\n",
    "- can't check available disk space\n",
    "- can't deal with unexpected folders\n",
    "\n",
    "<a id='v2'></a>\n",
    "### v.2 \n",
    "- Updated print messages\n",
    "\n",
    "<a id='v3'></a>\n",
    "### v.3\n",
    "- Restructured to unpack onto SSD\n",
    "- Developed functionality for pre-2005 patent data\n",
    "- Included S3 Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='User Guide'></a>\n",
    "## User Guide\n",
    "\n",
    "This program assumes that the patent data is in a .tar or .zip file format. These files can be accessed at https://www.google.com/googlebooks/uspto-patents-redbook.html. Note the .tar format was adopted for years 2011-present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Path Specification'></a>\n",
    "#### Path Specification\n",
    "\n",
    "Please replace Google Patent Data with the location of the existing or desired patent directory. Please be sure to omit the trailing /. If the patent data already exists, it should be organized into subfolders corresponding to its year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set path of raw data\n",
    "directory_path = 'Google Patent Data'\n",
    "\n",
    "#create folder if not there\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you do not wish to download relevant files through this program, please skip to 'Output Format'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests, urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates a list of all available files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scrape list of available files\n",
    "r = requests.get('https://www.google.com/googlebooks/uspto-patents-redbook.html')\n",
    "soup = BeautifulSoup(r.text, 'html.parser').find_all('a')\n",
    "tarlist = []\n",
    "for link in soup:\n",
    "    if ((link.has_attr('href') and '.tar' in link['href']) or (link.has_attr('href') and '.ZIP' in link['href'] and 'SUPP' not in link['href'])):\n",
    "        print ('\\''+link['href']+'\\''+',')\n",
    "        tarlist.append(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy the download links of the desired files into the following array, deleting the trailing comma at the end of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_to_download = ['http://storage.googleapis.com/patents/redbook/grants/2001/20011225.ZIP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From this point onward, you may wish to run all remaining cells. (EDIT: Beware compact version at end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#download selected files\n",
    "for downloadfile in files_to_download:\n",
    "    \n",
    "    year = downloadfile[53:57]\n",
    "    \n",
    "    if '.' in downloadfile[58:71]:\n",
    "        week = downloadfile[58:71]\n",
    "        week2 = downloadfile[58:66]\n",
    "    else:\n",
    "        week = downloadfile[58:70]\n",
    "        week2 = downloadfile[58:66]\n",
    "    \n",
    "    if os.path.exists(directory_path+'/'+year+'/'+week):\n",
    "        print (directory_path+'/'+year+'/'+week+' already exists.')\n",
    "    else:\n",
    "        if not os.path.exists(directory_path+'/'+year):\n",
    "            os.makedirs(directory_path+'/'+year)\n",
    "        print ('Downloading '+week+'...')\n",
    "        urllib.urlretrieve(downloadfile, directory_path+'/'+year+'/'+week)\n",
    "        print ('Downloading '+week+' complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Format\n",
    "\n",
    "The following list determines the filetypes that are kept by the unpacker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exts = ['.xml', '.sgm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also indicate whether you want to keep patent supplements (those which are in .xml format), design patents, DTDS, Plant and Reissue folders.\n",
    "\n",
    "##### WARNING: Currently only supports FALSE for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_supplements = False\n",
    "keep_Design = False\n",
    "keep_DTDS = False\n",
    "keep_Plant = False\n",
    "keep_Reissue = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Main Code'></a>\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tarfile, zipfile, shutil, time\n",
    "from unipath import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function that displays any directory tree\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "#calculates the size of a directory\n",
    "def get_size(start_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    print ('The directory has a size of '+str(total_size/1000000)+' MB.')\n",
    "\n",
    "#confirm patent directory\n",
    "list_files(directory_path)\n",
    "get_size(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](Data/Images/2.1 Directory Sample.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Extraction'></a>\n",
    "#### Before Proceeding\n",
    "If the tree above does not mirror the intended structure, please make sure you provided the correct directory path, or double check the directory on your local machine. As a reminder, it should be sorted in subfolders corresponding to the respective year.\n",
    "\n",
    "The subsequent code may take up to several hours to run. Even though there are further checks along the way, it is vital that the information up to this point is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "#unzip everything\n",
    "if not os.path.exists(directory_path+'/Unpacked Data'):\n",
    "    os.makedirs(directory_path+'/Unpacked Data')\n",
    "for root, dirs, files in os.walk(directory_path, topdown=True):\n",
    "    dirs[:] = [d for d in dirs if d not in 'Unpacked Data']\n",
    "    for currentdir in dirs: \n",
    "        print ('Creating unpacked '+currentdir+' directory.')\n",
    "        newdir_year = (directory_path+'/Unpacked Data/'+currentdir)\n",
    "        if not os.path.exists(newdir_year):\n",
    "            os.makedirs(newdir_year)\n",
    "        else:\n",
    "            pass\n",
    "        for root, dirs, files in os.walk(directory_path+'/'+currentdir):\n",
    "            for currentfile in files:\n",
    "                print ('Extracting '+currentfile+'...')\n",
    "                newfile = os.path.splitext(currentfile)[0]\n",
    "                newdir_week = (newdir_year+'/'+newfile)\n",
    "                if not os.path.exists(newdir_week):\n",
    "                    os.makedirs(newdir_week)\n",
    "                    if '.tar' in currentfile:\n",
    "                        tarfile = tarfile.open(name=(directory_path+'/'+currentdir+'/'+currentfile), mode='r')\n",
    "                        tarfile.extractall(path=newdir_week, members=None)\n",
    "                    elif '.ZIP' in currentfile:\n",
    "                        with zipfile.ZipFile(directory_path+'/'+currentdir+'/'+currentfile) as zf:\n",
    "                            zf.extractall(path=newdir_week, members=None)\n",
    "                    else:\n",
    "                        print ('File type not recognized.')\n",
    "                    print ('Extraction of '+currentfile+' complete.')\n",
    "                else:\n",
    "                    print ('Extracted '+currentfile+' already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the high-level output tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list_files(directory_path+'/Unpacked Data')\n",
    "# get_size(directory_path+'/Unpacked Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your preferences in 'Path Specification', the following code will delete unwanted data. If something goes wrong in the subsequent lines of code, please return to 'Before Proceeding' to reprocess the raw .tar file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning'></a>\n",
    "#### Current Configuration (v. 3) automatically deletes all unwanted folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#delete all non-text data and supplements\n",
    "for root, dirs, files in os.walk(directory_path+'/Unpacked Data/'+year+'/'+week2+'/'+week2):\n",
    "    for currentdir in dirs:\n",
    "        print currentdir\n",
    "        if ('UTIL' not in currentdir):\n",
    "                shutil.rmtree(root+'/'+currentdir)\n",
    "                print (root+'/'+currentdir+' deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "#     for currentdir in dirs:\n",
    "#         if (keep_Design == False and 'DESIGN' in currentdir):\n",
    "#             shutil.rmtree(root+'/'+currentdir)\n",
    "#             print (root+'/'+currentdir+' deleted.')\n",
    "#         if (keep_Plant == False and 'PLANT' in currentdir):\n",
    "#             shutil.rmtree(root+'/'+currentdir)\n",
    "#             print (root+'/'+currentdir+' deleted.')\n",
    "#         if (keep_Reissue == False and 'REISSUE' in currentdir):\n",
    "#             shutil.rmtree(root+'/'+currentdir)\n",
    "#             print (root+'/'+currentdir+' deleted.')\n",
    "#         if (keep_DTDS == False and 'DTDS' in currentdir):\n",
    "#             shutil.rmtree(root+'/'+currentdir)\n",
    "#             print (root+'/'+currentdir+' deleted.')\n",
    "#         if (keep_supplements == False and 'SUPP' in currentdir):\n",
    "#             shutil.rmtree(root+'/'+currentdir)\n",
    "#             print (root+'/'+currentdir+' deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "#unpack individual patents\n",
    "for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "    for currentfile in files:\n",
    "            with zipfile.ZipFile(root+'/'+currentfile) as z:\n",
    "                z.extractall(root)\n",
    "                newfile = os.path.splitext(currentfile)[0]\n",
    "                z.close()\n",
    "                os.remove(root+'/'+currentfile)\n",
    "                shutil.move(root+'/'+newfile, Path((Path(root).parent)).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "#clean up tree structure\n",
    "for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):       \n",
    "    for currentfile in files:\n",
    "        if any(currentfile.lower().endswith(ext) for ext in exts):\n",
    "            shutil.copy(root+'/'+currentfile, Path(root).parent)\n",
    "            os.remove(root+'/'+currentfile)\n",
    "        else:\n",
    "            os.remove(root+'/'+currentfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "#delete all anomalies\n",
    "for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "    for currentdir in dirs:\n",
    "        if not os.listdir(root+'/'+currentdir):\n",
    "            os.rmdir(root+'/'+currentdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your preferences in 'Output Format', the following code will delete unwanted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "if keep_supplements == False:\n",
    "    for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "        for currentfile in files:\n",
    "            if currentfile.count('-') > 1:\n",
    "                os.remove(root+'/'+currentfile)\n",
    "\n",
    "for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "    for currentdir in dirs:\n",
    "        if not os.listdir(root+'/'+currentdir):\n",
    "            os.rmdir(root+'/'+currentdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_files(directory_path+'/Unpacked Data')\n",
    "get_size(directory_path+'/Unpacked Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shutil.make_archive(directory_path+'/'+week2, 'zip', directory_path+'/Unpacked Data/'+year+'/'+week2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Uploading'></a>\n",
    "### Uploading to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create bucket to upload to\n",
    "s3.create_bucket(Bucket='ckpatents', CreateBucketConfiguration={\n",
    "    'LocationConstraint': 'us-west-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('ckpatents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "#upload to S3\n",
    "s3.Object('ckpatents', year+'/'+week2+'.zip').put(Body=open(directory_path+'/'+week2+'.zip', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set bucket attributes\n",
    "bucket.Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](Data/Images/2.1 S3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for SSD purposes, deletes all downloaded and unpacked files\n",
    "shutil.rmtree('Google Patent Data', ignore_errors=True)\n",
    "print ('Google Patent Data deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Compact Version'></a>\n",
    "# Compact Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code mirrors the format above, except that weeks are downloaded/unpacked/uploaded individually. This is useful for running on a machine with little storage space (i.e. local machine SSD). The code can run the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory_path = 'Google Patent Data'\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests, urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://www.google.com/googlebooks/uspto-patents-redbook.html')\n",
    "soup = BeautifulSoup(r.text, 'html.parser').find_all('a')\n",
    "tarlist = []\n",
    "for link in soup:\n",
    "    if ((link.has_attr('href') and '.tar' in link['href']) or (link.has_attr('href') and '.ZIP' in link['href'] and 'SUPP' not in link['href'])):\n",
    "        print ('\\''+link['href']+'\\''+',')\n",
    "        tarlist.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exts = ['.xml', '.sgm']\n",
    "keep_supplements = False\n",
    "keep_Design = False\n",
    "keep_DTDS = False\n",
    "keep_Plant = False\n",
    "keep_Reissue = False\n",
    "import tarfile, zipfile, shutil, time\n",
    "from unipath import Path\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('ckpatents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_to_download = ['http://storage.googleapis.com/patents/redbook/grants/2005/I20050111.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050118.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050125.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050201.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050208.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050215.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050222.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050301.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050308.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050315.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050322.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050329.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050405.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050412.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050419.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050426.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050503.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050510.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050517.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050524.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050531.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050607.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050614.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050621.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050628.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050705.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050712.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050719.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050726.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050802.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050809.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050816.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050823.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050830.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050906.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050913.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050920.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20050927.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051004.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051011.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051018.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051025.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051101.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051108.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051115.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051122.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051129.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051206.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051213.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051220.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2005/I20051227.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060221.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060228.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060307.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060314.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060321.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060328.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060404.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060411.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060418.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060425.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060502.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060509.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060516.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060523.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060530.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060606.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060613.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060620.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060627.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060704.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060711.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060718.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060725.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060801.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060808.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060815.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060822.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060829.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060905.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060912.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060919.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20060926.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061003.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061010.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061017.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061024.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061031.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061107.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061114.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061121.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061128.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061205.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061212.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061219.tar',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061219.ZIP',\n",
    "'http://storage.googleapis.com/patents/redbook/grants/2006/I20061226.ZIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for downloadfile in files_to_download:\n",
    "    \n",
    "    year = downloadfile[53:57]\n",
    "    \n",
    "    if '.' in downloadfile[58:71]:\n",
    "        week = downloadfile[58:71]\n",
    "        week2 = downloadfile[58:67]\n",
    "    else:\n",
    "        week = downloadfile[58:70]\n",
    "        week2 = downloadfile[58:66]\n",
    "    print week, week2\n",
    "    if os.path.exists(directory_path+'/'+year+'/'+week):\n",
    "        print (directory_path+'/'+year+'/'+week+' already exists.')\n",
    "    else:\n",
    "        if not os.path.exists(directory_path+'/'+year):\n",
    "            os.makedirs(directory_path+'/'+year)\n",
    "        print ('Downloading '+week+'...')\n",
    "        urllib.urlretrieve(downloadfile, directory_path+'/'+year+'/'+week)\n",
    "        print ('Downloading '+week+' complete.')\n",
    "        \n",
    "    #function that displays any directory tree\n",
    "    def list_files(startpath):\n",
    "        for root, dirs, files in os.walk(startpath):\n",
    "            level = root.replace(startpath, '').count(os.sep)\n",
    "            indent = ' ' * 4 * (level)\n",
    "            print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "            subindent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                print('{}{}'.format(subindent, f))\n",
    "\n",
    "    #calculates the size of a directory\n",
    "    def get_size(start_path):\n",
    "        total_size = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "            for f in filenames:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                total_size += os.path.getsize(fp)\n",
    "        print ('The directory has a size of '+str(total_size/1000000)+' MB.')\n",
    "\n",
    "    #confirm patent directory\n",
    "    list_files(directory_path)\n",
    "    get_size(directory_path)\n",
    "    \n",
    "    if not os.path.exists(directory_path+'/Unpacked Data'):\n",
    "        os.makedirs(directory_path+'/Unpacked Data')\n",
    "    for root, dirs, files in os.walk(directory_path, topdown=True):\n",
    "        dirs[:] = [d for d in dirs if d not in 'Unpacked Data']\n",
    "        for currentdir in dirs: \n",
    "            print ('Creating unpacked '+currentdir+' directory.')\n",
    "            newdir_year = (directory_path+'/Unpacked Data/'+currentdir)\n",
    "            if not os.path.exists(newdir_year):\n",
    "                os.makedirs(newdir_year)\n",
    "            else:\n",
    "                pass\n",
    "            for root, dirs, files in os.walk(directory_path+'/'+currentdir):\n",
    "                for currentfile in files:\n",
    "                    print ('Extracting '+currentfile+'...')\n",
    "                    newfile = os.path.splitext(currentfile)[0]\n",
    "                    newdir_week = (newdir_year+'/'+newfile)\n",
    "                    if not os.path.exists(newdir_week):\n",
    "                        os.makedirs(newdir_week)\n",
    "                        if '.tar' in currentfile:\n",
    "                            tarfile = tarfile.open(name=(directory_path+'/'+currentdir+'/'+currentfile), mode='r')\n",
    "                            tarfile.extractall(path=newdir_week, members=None)\n",
    "                        elif '.ZIP' in currentfile:\n",
    "                            with zipfile.ZipFile(directory_path+'/'+currentdir+'/'+currentfile) as zf:\n",
    "                                zf.extractall(path=newdir_week, members=None)\n",
    "                        else:\n",
    "                            print ('File type not recognized.')\n",
    "                        print ('Extraction of '+currentfile+' complete.')\n",
    "                    else:\n",
    "                        print ('Extracted '+currentfile+' already exists.')\n",
    "                        \n",
    "    for root, dirs, files in os.walk(directory_path+'/Unpacked Data/'+year+'/'+week2+'/'+week2):\n",
    "        for currentdir in dirs:\n",
    "            print currentdir\n",
    "            if ('UTIL' not in currentdir):\n",
    "                    shutil.rmtree(root+'/'+currentdir)\n",
    "                    print (root+'/'+currentdir+' deleted')\n",
    "                    \n",
    "    for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "        for currentfile in files:\n",
    "                with zipfile.ZipFile(root+'/'+currentfile) as z:\n",
    "                    z.extractall(root)\n",
    "                    newfile = os.path.splitext(currentfile)[0]\n",
    "                    z.close()\n",
    "                    os.remove(root+'/'+currentfile)\n",
    "                    shutil.move(root+'/'+newfile, Path((Path(root).parent)).parent)\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):       \n",
    "        for currentfile in files:\n",
    "            if any(currentfile.lower().endswith(ext) for ext in exts):\n",
    "                shutil.copy(root+'/'+currentfile, Path(root).parent)\n",
    "                os.remove(root+'/'+currentfile)\n",
    "            else:\n",
    "                os.remove(root+'/'+currentfile)\n",
    "                \n",
    "    for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "        for currentdir in dirs:\n",
    "            if not os.listdir(root+'/'+currentdir):\n",
    "                os.rmdir(root+'/'+currentdir)\n",
    "    \n",
    "    if keep_supplements == False:\n",
    "        for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "            for currentfile in files:\n",
    "                if currentfile.count('-') > 1:\n",
    "                    os.remove(root+'/'+currentfile)\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path+'/Unpacked Data'):\n",
    "        for currentdir in dirs:\n",
    "            if not os.listdir(root+'/'+currentdir):\n",
    "                os.rmdir(root+'/'+currentdir)\n",
    "                \n",
    "    shutil.make_archive(directory_path+'/'+week2, 'zip', directory_path+'/Unpacked Data/'+year+'/'+week2)\n",
    "    \n",
    "    s3.Object('ckpatents', year+'/'+week2+'.zip').put(Body=open(directory_path+'/'+week2+'.zip', 'rb'))\n",
    "    \n",
    "    shutil.rmtree('Google Patent Data', ignore_errors=True)\n",
    "    print ('Google Patent Data deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
